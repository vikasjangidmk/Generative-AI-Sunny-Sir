{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "WEAVIATE_CLUSTER = os.getenv(\"WEAVIATE_CLUSTER\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate client initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\weaviate\\warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.24.0. The latest version is 4.10.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_CLUSTER\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY)\n",
    ")\n",
    "\n",
    "print(\"Weaviate client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_14088\\207400733.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name,\n",
    "  #model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"RAG_From_Scratch.pdf\",extract_images=True)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content=\"Retrieval Augmented' Generation(RAG)\\nWhat you will learn:\\n1. Introduction of RAG.\\n2. RAG Architecture.\\ninqeshon\\n3. End to End RAG Pipeline.\\n4. Advantage Disadvantage of RAG.\\n5. Build RAG from Scratch.\\n6.Multimodal RAG:\\n7. RAG v/s Finetuning.\\n8.EvaluationMatricesof RAG.—\\n9. How to improve RAG system and Important research Paper.\\nGT=\\n2022\\nDPx\\nekn\\nL人m+'o+her tifo CDoc → De)\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='1.Introduction:WhatisRAG?\\nRAG is called Retrieval augmented generation, is an architecture used to help LLMs model like gpt- 4,\\nGemini, Gemma,LLAMA2, MISTRAL to provide a-better response by using relevant information from\\nadditional sources andreduces thechancethatllm-willleadtoincorrectinformation\\nor\\nRAG is a technique that enhances language model generation by incorporating external knowledge.This\\nis typically doneby retrievingrelevantinformation from alarge corpus of documents and usingthat\\ninformation to inform-the generation process.\\nor\\nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights\\n means it is not inside the training.\\nfrsq= hse\\n·)\\nWhy we should use RAG?\\n1:Limitedknowledgeaccess\\nLack of transparency: LLMs struggle to provide transparent or relevant information\\n3.Hallucinationsinanswers\\n2.RAG Architecture.\\n1. Ingestion\\n2. Retrieval\\n3. Generation\\nArchitecture\\nL0,1 0.3 0.33\\nQuestion\\ntextchunk\\nEmbedbingy -1\\ncResar\\nueryEmbedeling\\nBuild\\nEmlbedking-2\\nSemantic\\nL0,1 0.3 0.33\\nExtractData/\\ntextchunk2\\nIndex\\nLLMGenerative\\nContent\\n10.30.3]\\nemanticSearch\\ntextchuk3\\nEmbedbing -3\\n[90h050]\\nLO.5 0.4 0.5]\\nnowledlge\\nRanked\\nEmbedeing-10\\nBase\\ntextchunk10\\nResults\\nQetaiva/ -)  6btaintnq\\nfhe dt frm D\\nAuq ment \\nehching.inf。\\nusmq.\\n心。\\nPronP)\\ngehrihon\\n人M\\nLkshq \\ncpr-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='embedd inq\\ny(oar)\\n√echr.=)\\nicar\\n[%]\\nP2=)[x0]\\nP3e [oy]\\nX\\n(ysom\\nower\\n<rng\\nTueeh\\n0.S.\\nMan°\\n0.13\\nSimlM\\nn\\nCosrhie smilanly\\necuLrdia \\nJaccare\\nFruil.\\nApple\\n[T,r]=[]\\nuys了\\n[][]\\n: Tech\\nSimiM?\\nhp 0@\\nCoSme =) Cos 90°'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content=\"3.End toEnd RAGPipeline.\\nLet'ssummariestheprocessofRAG:\\nBasic RAG Pipeline\\nIngestion\\nDocuments\\nEmbeddings\\nChunks\\nIndex\\nRetrieval\\nSynthesis\\nIndex\\nTopK\\nQuery\\nLLM\\nResponse\\nRAG Flow:A StepbyStep Representation\\nFigure 1 demonstrates thestepsinvolvedinbuildinga RAGpipeline:\\nLegend\\nExtraction， chunking\\nEnterprise Data Store \\nDatabricks, Anyscale, LangChan\\nContextual data\\nAIWS,MictosoftGoogleOracieDatabricks,\\nSnowllake MongoDB,CouchoB,DataStax\\nNotion, Codae, Salesforoe, JiRA, eto\\nUser Query\\nMany others.\\nRetrieval engine\\nAction\\nText Storage DB\\nApp hosting\\nVercel,streamlit, Gradio,\\nreplit, fly.io\\nector Store /DB\\nQdrantPine\\nme,zl\\nEmbedding LLM\\na,P9N\\nOpenAiLCohere,\\nHuggingface\\nQuery\\nLLM hosting \\nMLOps\\nResponse\\nMLlow, W&B, Aim\\nConstruet prompt\\nLLM hosting\\nLangchein, Rebuff, DIY\\nOperALCohere,Anthropic\\nAzure, AWS, GCP\\nEnterprise App Automation\\nepiea asuodsa\\nZapier,JIRAtionod\\nAsana, Monday, email, etc\\nNvidia Guardrails\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content=\"A.).LetsunderstandtheIngestionprocess\\n1.Document:\\nIn a typical RAG pipeline, we have knowledge sources, such as Local Files, CSV, TSV, JSON, PDF, DOCs, Web pages,\\nXML,Databases,Cloud Storage,AnyRemoteLocation etc.\\n2. Chunking:\\nWe collect the data from yarious sources, split the data,into the chunks.\\nWhy chunking is required?\\nWe can fed the entire document alsobut why we are just passing the paragraph so here the reason is\\n1. LLM will rt be overloaded with information and\\nCcmini?.\\n)p9)\\n(GpT\\n2.Evenit ishaving somelimitsinterms oftokens\\nHow tofiguring out theIdeal ChunkSize\\nToo small a chunk won't provide you info. It is not sufficient and These chrunks can be defined either by a fixed size, such\\nas a specific number of characters, sentences or paragraphs.\\nLarger chunks might include irrelevant information,\\nintroducing noise and potentially\\nHeducing the retrieval accuracy.By\\ncontrollinft\\nthechunksize\\nRAGcanmaintainabalancebetweencomprehensivenessandprecision.\\nBased on these factor you can' decide the size of chunk.\\nDa+s - CorP.as\\n1. DataCharacteristics\\nChuiic -) Doce amenlh.\\ntdc\\n2.Retriever Constraints\\n(s+n+ece)\\n3.Memory and Computational Resources\\n4TaskRequirements\\n5.Experimentation\\n6. OverlapConsideration\\n@hanics =) fokeins (wora )\\nhttps://www.pinecone.io/learn/chunking-strategies/\\nSenter o ( Doc, Chan k)\\nWo -d (T)\\nParagraa\\n3. Embedding\\nAfter chunking we convert it into vector embedding. vector embeddingare numerical representations of the data.\\nTypes of embedding\\n1. Frequency based embedding\\nSe menh\\nBOW\\nmbeddrng\\nTF-IDF\\nN-GRAMS\\n>2. Neural Network based embeddings\\nWord2Vec\\nfasttext\\nBert\\nElmo\\nOPEN AI Embedding\\nGeminiEmbedding\\nToken level embedding vs Sentence level embedding.\\nCheckout the link: https: //huggingface.co/sentence-transformers\\nhttps://www.sbert.net/\\nwhich embedding model you need to select:\\nCheckouttheleaderboard:https://huggingface.co/spaces/mteb/leaderboard\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content=\"howsentencetransformersdiffercomparedtotoken-levelembeddingmodelssuchasBERT?\\nSentence transformers are specifically optimized for producing representations at the sentence level, focusing o\\ncapturing the overall semantics of sentences,which makes them particularly useful for tasks involving sentence similarit\\nand clustering. This contrasts with token-level models like BERT, which aremore focused on understanding an\\nrepresentingthemeaningofindividualtokenswithintheirwidercontext.\\n4. Vector Embedding Indexing\\ndimensionalvectordata,enablingfastsimilaritysearchesandnearestneighborqueries.\\nCheckout the link: https://www.datastax.com/guides/what-is-a-vector-index\\nAvectordatabase\\nindexes\\nandstores\\nvectorembeddings\\nvector indexing\\nfor fast retrieval\\nand similanitysearch.\\nIndex\\nJembecldlings\\n40.70.90.10.5\\n20.30.80.70.3\\n0.60.40.30.2\\nunstructuredl\\ndata\\n0.30.20.10.9\\ndatastructure\\nofteninclucingadistancemetric\\n5.Database or Retriever:\\nTheretriever herecould beanyof thefollowingdependingontheneed\\nVectordatabase:Avectordatabaseindexesandstoresvectorembeddingsforfastretrievalandsimilaritysearch,with\\ncapabilities like CRUD operations, metadata filtering, horizontal scaling, and serverless.\\nGraph database:\\nGraph databases are designed to represent and store data as graphs.This makes it easy to represent\\npeople, products, and events along with what ties them together. Search engines, logistics businesses, and social networks\\ntypically.usegraphdatabasestounderstandconnectionsintheirdata.\\nNodes are the primary entities in a graph database. Each node holds all data about a person, product, business, event, or\\nanother entity.\\nEdges are theconnecting parts of graph databases. They showsimilarities, relationships, and commonalities.You can\\ndefine the properties and weights of edges tofit your purpose.\\nPegularSQLdatabase:Offersstructureddatastorageandretrievalbutmightlackthesemanticflexibilityofvector\\ndatabases.\\nGraph databases bring you the fullpower of relationships in data.\\nVector databases arebest suited for managing and querying high-dimensional data in use cases that require similarity\\nsearches.\\nIt can behard tomake theMice andgowith eithergraph or vector technology foryour database.Withgenerative AI,\\nlarge language models (LLM), and real-time data playing an increasing part in modern applications, we're seeing an\\nincreaseincombined solutions\\nWith generative AI, large language models (LLM), and real-time data playing an increasing part in modern applications,\\nwe'reseeinganincreasein'combinedsolutions.\\nThis is why Nee4j recently added the ability to perform vector similarity search.They aim to make more sense of data and\\ngraph\\ncheckout thelink:https://superlinked.com/vector-db-comparison/\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content=\"B).Let'sUnderstandtheretrievalprocess\\nStandardnaiveapproach\\nIn a basic RAG Pipeline\\nDocuments\\nChunks\\nThesametextchunksused in\\nembeddingsandsynthesis.\\nEmbeddingforretrieval\\nSynthesis\\ntasks\\nTopK\\nretrieved\\nEmbeddings\\nchunks\\nLLM\\nResponse\\nBut:\\nEmbedding-basedretrievalworkswellwith\\nsmallertextchunks.\\nThe standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.\\nAdvantages:\\n·Simplicity and Efficiency\\n· Uniformity in Data Handling\\nDisadvantages:\\n·Limited Contextual Understanding\\n·PotentialforSuboptimalResponses\\nSentence-WindowRetrieval / Small-to-Large.Chunking\\nInSentence-windowretrieval\\npipeline\\ncontext\\nDocuments\\nChunks\\nsmaller\\nContextaroundthechunks\\nchunks\\naddedtotheretrievedones\\nEmbeddingforretrieval\\nSynthesis\\ntasks\\nTopK\\nretrieved\\nEmbeddings\\nchunks\\nLLM\\nResponse\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='During retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the\\nsentence withthefull surrounding context(using a static sentence-window around the context,implemented by\\nretrievingsentencessurroundingtheonebeingoriginallyretrieved)\\nSentence-windowretrieval\\nQuery:Whatarethe\\nconcernssurrounding\\ntheAMOC?\\nContinuousobservationof theAtlantic\\nmeridionaloverturning circulation（AMoc)\\nhasimproved theunderstanding of its\\nvariability（Frajka-Williams et al,2019),but\\nthereislowconfidenceinthequalificationof\\nAMOC changesinthe2oth centurybecause\\nof lowagreementinquantitative\\nWhat theLLM sees\\nreconstructed andsimulated trends.Direct\\nobservationalrecords since the mid-2000s\\nremaintooshorttodetermine therelative\\ncontributions of internal variability,natural\\nforcingandanthropogenictoAMocchange\\n(highconfidence).Overthe 2lstcentury\\nAMOCwill very likelydeclineforallSsp\\nEmbeddingLookup\\nscenariosbutwillnotinvolveanabrupt\\ncollapsebefore2100.3.2.2.4Sea IceChanges\\nSea ice is akey driver of polar marine life,\\nhosting unique ecosystems and affecting\\ndiversemarineorganismsandfoodwebs\\nthroughitsimpactonlightpenetrationsand\\nWhattheLLMsees\\nsuppliesof nutrientsandorganicmatter(\\nArrigo, 2014).\\nAdvantages:\\n·Enhanced Specificity in Retrieval\\n·Context-Rich Synthesis\\n·Balanced Approach\\nDisadvantages:\\n?Increased Complexity\\nAuto-mergingRetriever/HierarchicalRetriever\\nAuto-merging retrieval\\nChunk\\nChunk\\n(512)\\n(512)\\nChunk\\nChunk\\nChunk\\nChunk\\n(128)\\n(128)\\n(128)\\n(128)\\nChunk\\nChunk\\nChunk\\nChunk\\n(128)\\n(128)\\n(128)\\n(128)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content=\"Auto-mergingretrieval aimsto.combine(or merge)informationfrommultiple sources orsegments of text tocreate a\\nmore comprehensive and contextually relevant response to a query.This approach isparticularly useful when no single\\ndocument orsegmentfully answersthequerybutrather the answerliesin combininginformationfrommultiplesources.\\nIt allows smaller chunks to be merged into bigger parent chunks. It does this via the following steps:\\nDefineahierarchyofsmallerchunkslinkedtoparentchunks:\\nIf the set of smaller chunks linking to a parent chunk exceeds some threshold (say,cosine similarity),then“merge\\nsmaller chunks into thebigger parent chunk.\\nThe methodwillfinallyretrievetheparentchunkforbettercontext.\\nAdvantages:\\n·ComprehensiveContextualResponses\\n·ReducedFragmentation\\n· Dynamic Content Integration\\nDisadvantages:\\n·Complexity in Hierarchy and Threshold Management\\n·Risk of Overgeneralization\\n·Computational Intensity\\nEnsemble Retrieval and Re-Ranking\\nAll documerts\\nQuery\\nJerry Liu\\n@jeryjliue\\nMM？\\nVector\\nIf you 1) already know RAG basics, and 2) want to become a superstar Al\\nDB\\nengineer, then learn to build advanced RAG from scratch \\nExcited to launch a new category of @llamaindex tutorials on this exact\\ntopic：\\nlearn advanced IR/Al concepts\\ntacklemore complex userqueries\\nImprove accuracy + reduce hallucination\\nIn this first part, we teach youhow to build a RAG-Fusion pipeline:\\nenables dynamic retrieval over complex questions.\\nA two=stage ret\\nQuery generation/rewriting\\n2Ensemble retrieval\\n3Reciprocal RankFusion\\nAllthese steps are based off core retrieval principles, designed to\\nimprove precision/recall for different user queries.By learning this, you'll\\nswess gg ino Bujzjwgdo jo, suoynul/sjoo au uje\\nLearnByBullding.Al\\nUserQuery\\nRetriever1\\nRetriever2\\nRetriever3\\nChunk size:128\\nChunk size:512\\nChunk size:1024\\nChunk(128)\\nChunk(512)\\nChunk(128)\\nChunk(1024)\\nChunk（512)\\nChunk(1024)\\nReranker\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content=\"Let's Understand the augmentation andgeneration\\nResponseGeneration/Synthesis\\nUser Input: A user provides a query in natural language, seeking an answer or completion.\\nInformationRetrieval:Theretrievalmechanismscansthevectordatabasetoidentifysegmentsthataresemantically\\nsimilar to the user's query (which is also embedded).These segments are then given to the LLM to enrich its context for\\ngeneratingresponses.\\nCombining Data: The chosen data'segments from the database are combined with the user's initial query, creating an\\nexpandedprompt.\\nGenerating Text: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-\\nawareresponse.\\nThis process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting\\na response that is not only informative but also aligns with the user's original query, maintaining a natural and\\nconversationaltone.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content=\"Benefits of RAG:\\n·With-RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights, providing it\\naccesstoexternalknowledgebases.\\n· Improved relevance and accuracy:\\n·Handling open-domain queries:\\n·Reduced generation bias:\\n·Multi-modal capabilities:\\n·image captioning, content summarization\\n.Human-AI Collaboration:\\n·RAGdoesn'trequiremodelretraining,savingtime andcomputationalresources.\\nIn summary, RAG models are well-suited for applications where there's a lot of information available, but it's not neatly\\norganised or labelled.\\nDisadvantage:\\nRAG's performance depends on the comprehensiveness and correctness of the retriever's knowledge base.\\nInformationLoss\\nIf welook at the chain of processes in theRAG system:\\n1.Chunkingthe.textandgeneratingembeddingforthechunks\\n2.Retrievingthechunksbysemanticsimilaritysearch\\n3.Generateresponsebasedonthetextofthetop_kchunks\\nWayofcreatingaRAG.\\nRAG libraries and frameworks\\nBuild your onRAGfromscratch\\nLang chain\\nLlama Index.\\nHaystack: End-to-end RAG framework for document search provided by Deepset\\nREALM: Retrieval Augmented Language Model (REALM) training is a Google toolkit for\\nopen-domain question answering with RAG.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content=\"RAG(Retrieval Augmented Generation)Cheatsheet\\nStages in RAG:\\nKey Concepts:\\n1.Loading:\\n1.Nodes and Documents:\\n Imgort your data (text fles, PFs, databases, APIs using Lamaub’s extense range f\\nFundamental units in Llamatndex, where Documents encapsulate data sources and Nodes represent\\nconnectors.\\ndata “chunks* with associated metadata.\\n2.Indexing:\\n1.Connectors:\\nCreate searchable data structures, primarlythrouh vecterembeddings andmetadata strateies,\\nBridge arious dataseurce int the RAG frmwrk, transfrming theint odes and cume\\neapa xaas tuaa fuqeua\\n1.Indexes:\\n3.Storing:\\no Securely store your indexed data and metadata for quick access without the need to re-index.\\nmetadata.\\n4.Querying:\\n1.Embeddings:\\no Utilize LLMs and Llamalndex data structures for diverse querying techniques, including sub-\\nNumerical representations of data, facilitating the relevance fitering preces.\\nqueries and hybrid strategies.\\n1.Retrievers:\\n5.Evaluation:\\nDefine efficient retrieval strategles, ensuring the relevancy and efficiency of deta retrieval.\\n Continuouslyassess the effectiveness of your pipeline to ensure accuracy, faithflness, and\\n1.Routers:\\npaads asuodsai\\nManage the selectien f appropriate retrievers based on query speciis and metaata.\\n1.Node Postprocessors:\\nApply transformations or re-ranking logic to refine the set of retrieved nodes.\\n1.Response Synthesizers:\\nApplicationTypes:\\nCraft responses from the LLM, util\\n1.Query Engines:\\n For dinect question-answering over your data.\\n2.Chat Engines:\\nLlamalndex\\nSingleStore\\n Enables coversations with your data for an interactive experience.\\n3.Agents:\\nAutomated decision-makers that interact with external too\\nadaptable for complex tasks.\\nLangChain\\nOpenAl\\nConstructior\\nRetrieval\\nRelationalDBs\\nGraphDBs\\nVectorbBs\\nRarking\\nRefinerent\\nestien→\\nduestion→\\nGuestio\\nText-to-SQL\\nText-to-Cypher\\nSelf-query retriever\\nal Language to SQL\\nNatural Language to Cypher\\nAuto-generatemetadata\\nd/orSQLw/PGVector\\nquery Langugage for GraphDBs\\nfilters fron query\\nRe-Rank,RankGPT,RAG-Fusion\\nCRAG\\nQuery\\nTranslation\\nRank or filter/compress docu\\nments based on relevance\\nQuery Decomposition\\nPsuedo\\neuments\\nAetiveretieval\\nCRAG\\nQuestion\\nQuestion\\nMulti-query, Step-back, RAG-Fusion\\nHyDE\\nRe-retrieve and/or retrive fron new data sources\\n(e.g.,wb）if retrieved documents aze not relevant\\nDecompose or re-phrase the input question\\nHypothetical documents\\nyeog\\nDiagramcreditLangchain\\nQuestion\\nAnswer\\nRelationalDB\\nDocuments\\nSteveNouri\\npt\\nVeetorstore\\nPrompt#2\\nLet LLM choose DB based\\nEnbed question and choose\\non the question\\nprompt based on sinilarity\\nIncl\\nGeneration\\nChunk Optimization\\nMulti-representation\\nHeirachicalIndeing\\nActiveretieval\\nCharecters\\nSectiors\\nSenant'ic\\nT'0]\\nDeliniters\\nSelf-RAG, RRR\\nSenantic Splitter\\nParent Document,Dense X\\nFine-tuning,ColBERT\\nRAPTOR\\nOptimize chunk size\\nConvert documents into compact\\nDomain-specific and/or\\nTree of document sumnarization\\nUse generation quality to inform\\nused for eabedding\\nretrieval units （e.g.,a sunmary)\\nadvanced enbedding model.s\\nat various abstraction Levels\\nquestion re-writing and/or\\nre-retrieval of documents\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content=\"Retrieval Augmented' Generation(RAG)\\nWhat you will learn:\\n1. Introduction of RAG.\\n2. RAG Architecture.\\ninqeshon\\n3. End to End RAG Pipeline.\\n4. Advantage Disadvantage of RAG.\\n5. Build RAG from Scratch.\\n6.Multimodal RAG:\\n7. RAG v/s Finetuning.\\n8.EvaluationMatricesof RAG.—\\n9. How to improve RAG system and Important research Paper.\\nGT=\\n2022\\nDPx\\nekn\\nL人m+'o+her tifo CDoc → De)\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='1.Introduction:WhatisRAG?\\nRAG is called Retrieval augmented generation, is an architecture used to help LLMs model like gpt- 4,\\nGemini, Gemma,LLAMA2, MISTRAL to provide a-better response by using relevant information from\\nadditional sources andreduces thechancethatllm-willleadtoincorrectinformation\\nor\\nRAG is a technique that enhances language model generation by incorporating external knowledge.This\\nis typically doneby retrievingrelevantinformation from alarge corpus of documents and usingthat'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='information to inform-the generation process.\\nor\\nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights\\n means it is not inside the training.\\nfrsq= hse\\n·)\\nWhy we should use RAG?\\n1:Limitedknowledgeaccess\\nLack of transparency: LLMs struggle to provide transparent or relevant information\\n3.Hallucinationsinanswers\\n2.RAG Architecture.\\n1. Ingestion\\n2. Retrieval\\n3. Generation\\nArchitecture\\nL0,1 0.3 0.33\\nQuestion\\ntextchunk\\nEmbedbingy -1\\ncResar'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='cResar\\nueryEmbedeling\\nBuild\\nEmlbedking-2\\nSemantic\\nL0,1 0.3 0.33\\nExtractData/\\ntextchunk2\\nIndex\\nLLMGenerative\\nContent\\n10.30.3]\\nemanticSearch\\ntextchuk3\\nEmbedbing -3\\n[90h050]\\nLO.5 0.4 0.5]\\nnowledlge\\nRanked\\nEmbedeing-10\\nBase\\ntextchunk10\\nResults\\nQetaiva/ -)  6btaintnq\\nfhe dt frm D\\nAuq ment \\nehching.inf。\\nusmq.\\n心。\\nPronP)\\ngehrihon\\n人M\\nLkshq \\ncpr-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='embedd inq\\ny(oar)\\n√echr.=)\\nicar\\n[%]\\nP2=)[x0]\\nP3e [oy]\\nX\\n(ysom\\nower\\n<rng\\nTueeh\\n0.S.\\nMan°\\n0.13\\nSimlM\\nn\\nCosrhie smilanly\\necuLrdia \\nJaccare\\nFruil.\\nApple\\n[T,r]=[]\\nuys了\\n[][]\\n: Tech\\nSimiM?\\nhp 0@\\nCoSme =) Cos 90°'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content=\"3.End toEnd RAGPipeline.\\nLet'ssummariestheprocessofRAG:\\nBasic RAG Pipeline\\nIngestion\\nDocuments\\nEmbeddings\\nChunks\\nIndex\\nRetrieval\\nSynthesis\\nIndex\\nTopK\\nQuery\\nLLM\\nResponse\\nRAG Flow:A StepbyStep Representation\\nFigure 1 demonstrates thestepsinvolvedinbuildinga RAGpipeline:\\nLegend\\nExtraction， chunking\\nEnterprise Data Store \\nDatabricks, Anyscale, LangChan\\nContextual data\\nAIWS,MictosoftGoogleOracieDatabricks,\\nSnowllake MongoDB,CouchoB,DataStax\\nNotion, Codae, Salesforoe, JiRA, eto\\nUser Query\\nMany others.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='Many others.\\nRetrieval engine\\nAction\\nText Storage DB\\nApp hosting\\nVercel,streamlit, Gradio,\\nreplit, fly.io\\nector Store /DB\\nQdrantPine\\nme,zl\\nEmbedding LLM\\na,P9N\\nOpenAiLCohere,\\nHuggingface\\nQuery\\nLLM hosting \\nMLOps\\nResponse\\nMLlow, W&B, Aim\\nConstruet prompt\\nLLM hosting\\nLangchein, Rebuff, DIY\\nOperALCohere,Anthropic\\nAzure, AWS, GCP\\nEnterprise App Automation\\nepiea asuodsa\\nZapier,JIRAtionod\\nAsana, Monday, email, etc\\nNvidia Guardrails'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='A.).LetsunderstandtheIngestionprocess\\n1.Document:\\nIn a typical RAG pipeline, we have knowledge sources, such as Local Files, CSV, TSV, JSON, PDF, DOCs, Web pages,\\nXML,Databases,Cloud Storage,AnyRemoteLocation etc.\\n2. Chunking:\\nWe collect the data from yarious sources, split the data,into the chunks.\\nWhy chunking is required?\\nWe can fed the entire document alsobut why we are just passing the paragraph so here the reason is\\n1. LLM will rt be overloaded with information and\\nCcmini?.\\n)p9)\\n(GpT'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content=\"Ccmini?.\\n)p9)\\n(GpT\\n2.Evenit ishaving somelimitsinterms oftokens\\nHow tofiguring out theIdeal ChunkSize\\nToo small a chunk won't provide you info. It is not sufficient and These chrunks can be defined either by a fixed size, such\\nas a specific number of characters, sentences or paragraphs.\\nLarger chunks might include irrelevant information,\\nintroducing noise and potentially\\nHeducing the retrieval accuracy.By\\ncontrollinft\\nthechunksize\\nRAGcanmaintainabalancebetweencomprehensivenessandprecision.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content=\"Based on these factor you can' decide the size of chunk.\\nDa+s - CorP.as\\n1. DataCharacteristics\\nChuiic -) Doce amenlh.\\ntdc\\n2.Retriever Constraints\\n(s+n+ece)\\n3.Memory and Computational Resources\\n4TaskRequirements\\n5.Experimentation\\n6. OverlapConsideration\\n@hanics =) fokeins (wora )\\nhttps://www.pinecone.io/learn/chunking-strategies/\\nSenter o ( Doc, Chan k)\\nWo -d (T)\\nParagraa\\n3. Embedding\\nAfter chunking we convert it into vector embedding. vector embeddingare numerical representations of the data.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='Types of embedding\\n1. Frequency based embedding\\nSe menh\\nBOW\\nmbeddrng\\nTF-IDF\\nN-GRAMS\\n>2. Neural Network based embeddings\\nWord2Vec\\nfasttext\\nBert\\nElmo\\nOPEN AI Embedding\\nGeminiEmbedding\\nToken level embedding vs Sentence level embedding.\\nCheckout the link: https: //huggingface.co/sentence-transformers\\nhttps://www.sbert.net/\\nwhich embedding model you need to select:\\nCheckouttheleaderboard:https://huggingface.co/spaces/mteb/leaderboard'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='howsentencetransformersdiffercomparedtotoken-levelembeddingmodelssuchasBERT?\\nSentence transformers are specifically optimized for producing representations at the sentence level, focusing o\\ncapturing the overall semantics of sentences,which makes them particularly useful for tasks involving sentence similarit\\nand clustering. This contrasts with token-level models like BERT, which aremore focused on understanding an\\nrepresentingthemeaningofindividualtokenswithintheirwidercontext.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='4. Vector Embedding Indexing\\ndimensionalvectordata,enablingfastsimilaritysearchesandnearestneighborqueries.\\nCheckout the link: https://www.datastax.com/guides/what-is-a-vector-index\\nAvectordatabase\\nindexes\\nandstores\\nvectorembeddings\\nvector indexing\\nfor fast retrieval\\nand similanitysearch.\\nIndex\\nJembecldlings\\n40.70.90.10.5\\n20.30.80.70.3\\n0.60.40.30.2\\nunstructuredl\\ndata\\n0.30.20.10.9\\ndatastructure\\nofteninclucingadistancemetric\\n5.Database or Retriever:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Theretriever herecould beanyof thefollowingdependingontheneed\\nVectordatabase:Avectordatabaseindexesandstoresvectorembeddingsforfastretrievalandsimilaritysearch,with\\ncapabilities like CRUD operations, metadata filtering, horizontal scaling, and serverless.\\nGraph database:\\nGraph databases are designed to represent and store data as graphs.This makes it easy to represent\\npeople, products, and events along with what ties them together. Search engines, logistics businesses, and social networks'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='typically.usegraphdatabasestounderstandconnectionsintheirdata.\\nNodes are the primary entities in a graph database. Each node holds all data about a person, product, business, event, or\\nanother entity.\\nEdges are theconnecting parts of graph databases. They showsimilarities, relationships, and commonalities.You can\\ndefine the properties and weights of edges tofit your purpose.\\nPegularSQLdatabase:Offersstructureddatastorageandretrievalbutmightlackthesemanticflexibilityofvector\\ndatabases.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content=\"databases.\\nGraph databases bring you the fullpower of relationships in data.\\nVector databases arebest suited for managing and querying high-dimensional data in use cases that require similarity\\nsearches.\\nIt can behard tomake theMice andgowith eithergraph or vector technology foryour database.Withgenerative AI,\\nlarge language models (LLM), and real-time data playing an increasing part in modern applications, we're seeing an\\nincreaseincombined solutions\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content=\"With generative AI, large language models (LLM), and real-time data playing an increasing part in modern applications,\\nwe'reseeinganincreasein'combinedsolutions.\\nThis is why Nee4j recently added the ability to perform vector similarity search.They aim to make more sense of data and\\ngraph\\ncheckout thelink:https://superlinked.com/vector-db-comparison/\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content=\"B).Let'sUnderstandtheretrievalprocess\\nStandardnaiveapproach\\nIn a basic RAG Pipeline\\nDocuments\\nChunks\\nThesametextchunksused in\\nembeddingsandsynthesis.\\nEmbeddingforretrieval\\nSynthesis\\ntasks\\nTopK\\nretrieved\\nEmbeddings\\nchunks\\nLLM\\nResponse\\nBut:\\nEmbedding-basedretrievalworkswellwith\\nsmallertextchunks.\\nThe standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.\\nAdvantages:\\n·Simplicity and Efficiency\\n· Uniformity in Data Handling\\nDisadvantages:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Disadvantages:\\n·Limited Contextual Understanding\\n·PotentialforSuboptimalResponses\\nSentence-WindowRetrieval / Small-to-Large.Chunking\\nInSentence-windowretrieval\\npipeline\\ncontext\\nDocuments\\nChunks\\nsmaller\\nContextaroundthechunks\\nchunks\\naddedtotheretrievedones\\nEmbeddingforretrieval\\nSynthesis\\ntasks\\nTopK\\nretrieved\\nEmbeddings\\nchunks\\nLLM\\nResponse'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='During retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the\\nsentence withthefull surrounding context(using a static sentence-window around the context,implemented by\\nretrievingsentencessurroundingtheonebeingoriginallyretrieved)\\nSentence-windowretrieval\\nQuery:Whatarethe\\nconcernssurrounding\\ntheAMOC?\\nContinuousobservationof theAtlantic\\nmeridionaloverturning circulation（AMoc)\\nhasimproved theunderstanding of its'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='variability（Frajka-Williams et al,2019),but\\nthereislowconfidenceinthequalificationof\\nAMOC changesinthe2oth centurybecause\\nof lowagreementinquantitative\\nWhat theLLM sees\\nreconstructed andsimulated trends.Direct\\nobservationalrecords since the mid-2000s\\nremaintooshorttodetermine therelative\\ncontributions of internal variability,natural\\nforcingandanthropogenictoAMocchange\\n(highconfidence).Overthe 2lstcentury\\nAMOCwill very likelydeclineforallSsp\\nEmbeddingLookup\\nscenariosbutwillnotinvolveanabrupt'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='collapsebefore2100.3.2.2.4Sea IceChanges\\nSea ice is akey driver of polar marine life,\\nhosting unique ecosystems and affecting\\ndiversemarineorganismsandfoodwebs\\nthroughitsimpactonlightpenetrationsand\\nWhattheLLMsees\\nsuppliesof nutrientsandorganicmatter(\\nArrigo, 2014).\\nAdvantages:\\n·Enhanced Specificity in Retrieval\\n·Context-Rich Synthesis\\n·Balanced Approach\\nDisadvantages:\\n?Increased Complexity\\nAuto-mergingRetriever/HierarchicalRetriever\\nAuto-merging retrieval\\nChunk\\nChunk\\n(512)\\n(512)\\nChunk\\nChunk'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='(512)\\nChunk\\nChunk\\nChunk\\nChunk\\n(128)\\n(128)\\n(128)\\n(128)\\nChunk\\nChunk\\nChunk\\nChunk\\n(128)\\n(128)\\n(128)\\n(128)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Auto-mergingretrieval aimsto.combine(or merge)informationfrommultiple sources orsegments of text tocreate a\\nmore comprehensive and contextually relevant response to a query.This approach isparticularly useful when no single\\ndocument orsegmentfully answersthequerybutrather the answerliesin combininginformationfrommultiplesources.\\nIt allows smaller chunks to be merged into bigger parent chunks. It does this via the following steps:\\nDefineahierarchyofsmallerchunkslinkedtoparentchunks:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='If the set of smaller chunks linking to a parent chunk exceeds some threshold (say,cosine similarity),then“merge\\nsmaller chunks into thebigger parent chunk.\\nThe methodwillfinallyretrievetheparentchunkforbettercontext.\\nAdvantages:\\n·ComprehensiveContextualResponses\\n·ReducedFragmentation\\n· Dynamic Content Integration\\nDisadvantages:\\n·Complexity in Hierarchy and Threshold Management\\n·Risk of Overgeneralization\\n·Computational Intensity\\nEnsemble Retrieval and Re-Ranking\\nAll documerts\\nQuery\\nJerry Liu'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Query\\nJerry Liu\\n@jeryjliue\\nMM？\\nVector\\nIf you 1) already know RAG basics, and 2) want to become a superstar Al\\nDB\\nengineer, then learn to build advanced RAG from scratch \\nExcited to launch a new category of @llamaindex tutorials on this exact\\ntopic：\\nlearn advanced IR/Al concepts\\ntacklemore complex userqueries\\nImprove accuracy + reduce hallucination\\nIn this first part, we teach youhow to build a RAG-Fusion pipeline:\\nenables dynamic retrieval over complex questions.\\nA two=stage ret'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content=\"A two=stage ret\\nQuery generation/rewriting\\n2Ensemble retrieval\\n3Reciprocal RankFusion\\nAllthese steps are based off core retrieval principles, designed to\\nimprove precision/recall for different user queries.By learning this, you'll\\nswess gg ino Bujzjwgdo jo, suoynul/sjoo au uje\\nLearnByBullding.Al\\nUserQuery\\nRetriever1\\nRetriever2\\nRetriever3\\nChunk size:128\\nChunk size:512\\nChunk size:1024\\nChunk(128)\\nChunk(512)\\nChunk(128)\\nChunk(1024)\\nChunk（512)\\nChunk(1024)\\nReranker\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content=\"Let's Understand the augmentation andgeneration\\nResponseGeneration/Synthesis\\nUser Input: A user provides a query in natural language, seeking an answer or completion.\\nInformationRetrieval:Theretrievalmechanismscansthevectordatabasetoidentifysegmentsthataresemantically\\nsimilar to the user's query (which is also embedded).These segments are then given to the LLM to enrich its context for\\ngeneratingresponses.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content=\"Combining Data: The chosen data'segments from the database are combined with the user's initial query, creating an\\nexpandedprompt.\\nGenerating Text: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-\\nawareresponse.\\nThis process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content=\"a response that is not only informative but also aligns with the user's original query, maintaining a natural and\\nconversationaltone.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content=\"Benefits of RAG:\\n·With-RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights, providing it\\naccesstoexternalknowledgebases.\\n· Improved relevance and accuracy:\\n·Handling open-domain queries:\\n·Reduced generation bias:\\n·Multi-modal capabilities:\\n·image captioning, content summarization\\n.Human-AI Collaboration:\\n·RAGdoesn'trequiremodelretraining,savingtime andcomputationalresources.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content=\"In summary, RAG models are well-suited for applications where there's a lot of information available, but it's not neatly\\norganised or labelled.\\nDisadvantage:\\nRAG's performance depends on the comprehensiveness and correctness of the retriever's knowledge base.\\nInformationLoss\\nIf welook at the chain of processes in theRAG system:\\n1.Chunkingthe.textandgeneratingembeddingforthechunks\\n2.Retrievingthechunksbysemanticsimilaritysearch\\n3.Generateresponsebasedonthetextofthetop_kchunks\\nWayofcreatingaRAG.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='WayofcreatingaRAG.\\nRAG libraries and frameworks\\nBuild your onRAGfromscratch\\nLang chain\\nLlama Index.\\nHaystack: End-to-end RAG framework for document search provided by Deepset\\nREALM: Retrieval Augmented Language Model (REALM) training is a Google toolkit for\\nopen-domain question answering with RAG.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='RAG(Retrieval Augmented Generation)Cheatsheet\\nStages in RAG:\\nKey Concepts:\\n1.Loading:\\n1.Nodes and Documents:\\n Imgort your data (text fles, PFs, databases, APIs using Lamaub’s extense range f\\nFundamental units in Llamatndex, where Documents encapsulate data sources and Nodes represent\\nconnectors.\\ndata “chunks* with associated metadata.\\n2.Indexing:\\n1.Connectors:\\nCreate searchable data structures, primarlythrouh vecterembeddings andmetadata strateies,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Bridge arious dataseurce int the RAG frmwrk, transfrming theint odes and cume\\neapa xaas tuaa fuqeua\\n1.Indexes:\\n3.Storing:\\no Securely store your indexed data and metadata for quick access without the need to re-index.\\nmetadata.\\n4.Querying:\\n1.Embeddings:\\no Utilize LLMs and Llamalndex data structures for diverse querying techniques, including sub-\\nNumerical representations of data, facilitating the relevance fitering preces.\\nqueries and hybrid strategies.\\n1.Retrievers:\\n5.Evaluation:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='5.Evaluation:\\nDefine efficient retrieval strategles, ensuring the relevancy and efficiency of deta retrieval.\\n Continuouslyassess the effectiveness of your pipeline to ensure accuracy, faithflness, and\\n1.Routers:\\npaads asuodsai\\nManage the selectien f appropriate retrievers based on query speciis and metaata.\\n1.Node Postprocessors:\\nApply transformations or re-ranking logic to refine the set of retrieved nodes.\\n1.Response Synthesizers:\\nApplicationTypes:\\nCraft responses from the LLM, util'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='1.Query Engines:\\n For dinect question-answering over your data.\\n2.Chat Engines:\\nLlamalndex\\nSingleStore\\n Enables coversations with your data for an interactive experience.\\n3.Agents:\\nAutomated decision-makers that interact with external too\\nadaptable for complex tasks.\\nLangChain\\nOpenAl\\nConstructior\\nRetrieval\\nRelationalDBs\\nGraphDBs\\nVectorbBs\\nRarking\\nRefinerent\\nestien→\\nduestion→\\nGuestio\\nText-to-SQL\\nText-to-Cypher\\nSelf-query retriever\\nal Language to SQL\\nNatural Language to Cypher'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Auto-generatemetadata\\nd/orSQLw/PGVector\\nquery Langugage for GraphDBs\\nfilters fron query\\nRe-Rank,RankGPT,RAG-Fusion\\nCRAG\\nQuery\\nTranslation\\nRank or filter/compress docu\\nments based on relevance\\nQuery Decomposition\\nPsuedo\\neuments\\nAetiveretieval\\nCRAG\\nQuestion\\nQuestion\\nMulti-query, Step-back, RAG-Fusion\\nHyDE\\nRe-retrieve and/or retrive fron new data sources\\n(e.g.,wb）if retrieved documents aze not relevant\\nDecompose or re-phrase the input question\\nHypothetical documents\\nyeog\\nDiagramcreditLangchain'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content=\"Question\\nAnswer\\nRelationalDB\\nDocuments\\nSteveNouri\\npt\\nVeetorstore\\nPrompt#2\\nLet LLM choose DB based\\nEnbed question and choose\\non the question\\nprompt based on sinilarity\\nIncl\\nGeneration\\nChunk Optimization\\nMulti-representation\\nHeirachicalIndeing\\nActiveretieval\\nCharecters\\nSectiors\\nSenant'ic\\nT'0]\\nDeliniters\\nSelf-RAG, RRR\\nSenantic Splitter\\nParent Document,Dense X\\nFine-tuning,ColBERT\\nRAPTOR\\nOptimize chunk size\\nConvert documents into compact\\nDomain-specific and/or\\nTree of document sumnarization\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Use generation quality to inform\\nused for eabedding\\nretrieval units （e.g.,a sunmary)\\nadvanced enbedding model.s\\nat various abstraction Levels\\nquestion re-writing and/or\\nre-retrieval of documents')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "vector_db = Weaviate.from_documents(\n",
    "    docs,embeddings, client=client,by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 11, 'page_label': '12', 'producer': 'PyPDF', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12}, page_content='RAG(Retrieval Augmented Generation)Cheatsheet\\nStages in RAG:\\nKey Concepts:\\n1.Loading:\\n1.Nodes and Documents:\\n Imgort your data (text fles, PFs, databases, APIs using Lamaub’s extense range f\\nFundamental units in Llamatndex, where Documents encapsulate data sources and Nodes represent\\nconnectors.\\ndata “chunks* with associated metadata.\\n2.Indexing:\\n1.Connectors:\\nCreate searchable data structures, primarlythrouh vecterembeddings andmetadata strateies,'), Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 10, 'page_label': '11', 'producer': 'PyPDF', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12}, page_content=\"In summary, RAG models are well-suited for applications where there's a lot of information available, but it's not neatly\\norganised or labelled.\\nDisadvantage:\\nRAG's performance depends on the comprehensiveness and correctness of the retriever's knowledge base.\\nInformationLoss\\nIf welook at the chain of processes in theRAG system:\\n1.Chunkingthe.textandgeneratingembeddingforthechunks\\n2.Retrievingthechunksbysemanticsimilaritysearch\\n3.Generateresponsebasedonthetextofthetop_kchunks\\nWayofcreatingaRAG.\"), Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 1, 'page_label': '2', 'producer': 'PyPDF', 'source': 'RAG_From_Scratch.pdf', 'total_pages': 12}, page_content='1.Introduction:WhatisRAG?\\nRAG is called Retrieval augmented generation, is an architecture used to help LLMs model like gpt- 4,\\nGemini, Gemma,LLAMA2, MISTRAL to provide a-better response by using relevant information from\\nadditional sources andreduces thechancethatllm-willleadtoincorrectinformation\\nor\\nRAG is a technique that enhances language model generation by incorporating external knowledge.This\\nis typically doneby retrievingrelevantinformation from alarge corpus of documents and usingthat')]\n"
     ]
    }
   ],
   "source": [
    "print(vector_db.similarity_search(\"what is rag?\", k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG(Retrieval Augmented Generation)Cheatsheet\n",
      "Stages in RAG:\n",
      "Key Concepts:\n",
      "1.Loading:\n",
      "1.Nodes and Documents:\n",
      " Imgort your data (text fles, PFs, databases, APIs using Lamaub’s extense range f\n",
      "Fundamental units in Llamatndex, where Documents encapsulate data sources and Nodes represent\n",
      "connectors.\n",
      "data “chunks* with associated metadata.\n",
      "2.Indexing:\n",
      "1.Connectors:\n",
      "Create searchable data structures, primarlythrouh vecterembeddings andmetadata strateies,\n"
     ]
    }
   ],
   "source": [
    "print(vector_db.similarity_search(\"what is rag?\", k=3)[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In summary, RAG models are well-suited for applications where there's a lot of information available, but it's not neatly\n",
      "organised or labelled.\n",
      "Disadvantage:\n",
      "RAG's performance depends on the comprehensiveness and correctness of the retriever's knowledge base.\n",
      "InformationLoss\n",
      "If welook at the chain of processes in theRAG system:\n",
      "1.Chunkingthe.textandgeneratingembeddingforthechunks\n",
      "2.Retrievingthechunksbysemanticsimilaritysearch\n",
      "3.Generateresponsebasedonthetextofthetop_kchunks\n",
      "WayofcreatingaRAG.\n"
     ]
    }
   ],
   "source": [
    "print(vector_db.similarity_search(\"what is rag?\", k=3)[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template=\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use ten sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "huggingfacehub_api_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if huggingfacehub_api_token:\n",
    "    print(\"Token loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load token. Check .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mistralai/Mistral-Nemo-Instruct-2407\n",
    "#mistralai/Mistral-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_kwargs={\"temperature\":1, \"max_length\":180}\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407 (Request ID: 3Tnrs8)\n\n401 Unauthorized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is rag system?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_core\\runnables\\base.py:3016\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3014\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3015\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3016\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    396\u001b[0m         )\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     ]\n\u001b[1;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    964\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 784\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    785\u001b[0m                 prompts,\n\u001b[0;32m    786\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    787\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    788\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    789\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    790\u001b[0m             )\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    793\u001b[0m         )\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1523\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1522\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1523\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1526\u001b[0m     )\n\u001b[0;32m   1527\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:138\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    136\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 138\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[0;32m    131\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:272\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m url \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mbuild_url(provider_helper\u001b[38;5;241m.\u001b[39mmap_model(model))\n\u001b[0;32m    271\u001b[0m headers \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_headers(headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:327\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\OneDrive\\Desktop\\Generative-AI-Sunny-Sir\\genai\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407 (Request ID: 3Tnrs8)\n\n401 Unauthorized"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"what is rag system?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
