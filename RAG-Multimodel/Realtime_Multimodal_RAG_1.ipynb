{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements=partition_pdf(\n",
    "    filename=\"data1/cj.pdf\",                  # mandatory\n",
    "    strategy=\"hi_res\",                                 # mandatory to use ``hi_res`` strategy\n",
    "    extract_images_in_pdf=True,                       # mandatory to set as ``True``\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],          # optional\n",
    "    extract_image_block_to_payload=False,                  # optional\n",
    "    extract_image_block_output_dir=\"extracted_data\",  # optional - only works when ``extract_image_block_to_payload=False``\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Header at 0x1d6aca48790>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad20b460>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75d3c0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75c9d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75c910>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bf0a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad20b9a0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa1d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7aa830>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7aa650>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7aa260>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7aaad0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad20b610>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7bef20>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7ab460>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7ab5b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7ab700>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7ab850>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7ab9a0>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ae7abaf0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7abc40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7abee0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75c1c0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75c070>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ae75c310>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75c460>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ae75c5b0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75c700>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75c9a0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75caf0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75cc40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75cd90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75cee0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae75d030>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae75d180>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75d420>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7befb0>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae75d570>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75d810>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7bee30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75d6c0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75dc00>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75dd50>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75dea0>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae75dff0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75e140>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae75e290>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae75e3e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae75e530>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75e7d0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75e680>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75e920>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75ea70>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75ebc0>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae75ed10>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x1d6ae75ee60>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae75efb0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75f100>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75f250>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75f4f0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75f640>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae75f790>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae75f8e0>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae75fa30>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae75fb80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae75fe20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bc100>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7beef0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7beec0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bf5b0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bf400>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7bf4c0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7bc070>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7bc8e0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7bca30>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bcb80>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae7bccd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bce20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bd0c0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bd210>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae7bd360>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bd4b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bd600>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bd750>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bd8a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bdb40>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7bcf70>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7bd9f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bdf30>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7bed10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7be080>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7aaa40>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7aab60>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae7aa680>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae7be1d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7be320>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7be470>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7be5c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7be710>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7be860>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7be9b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7beb00>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ae7bf040>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bf190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bf2e0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7bfac0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7bfc10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bfd60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7bfeb0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7ab400>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7ab490>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7ab3a0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7abd90>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7abd60>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7abf40>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7abe50>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa2c0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa410>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa8f0>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ae784040>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa920>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa890>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aaaa0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aa800>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7aaa70>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad20b3d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad20b340>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad20b220>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad543a00>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad543e80>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad543ca0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7842e0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad543f40>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad543c10>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae784190>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae786260>,\n",
       " <unstructured.documents.elements.Header at 0x1d6ae7863b0>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ae786500>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae786650>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7867a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7868f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae786b90>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ad543bb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae786a40>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Header=[]\n",
    "Footer=[]\n",
    "Title=[]\n",
    "NarrativeText=[]\n",
    "Text=[]\n",
    "ListItem=[]\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
    "            Header.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "            Footer.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "            Title.append(str(element))\n",
    "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "            NarrativeText.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "            Text.append(str(element))\n",
    "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "            ListItem.append(str(element))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=[]\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "            img.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=[]\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tab.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reported Revenue Next Quarter Rev Actual Consensus A Guidance Consensus A ServiceNow $2,957M $2959.6M (0.1%) NA $3122.0M NA AppFolio $204.0M $199.5M 2.3% NA $217.3M NA Atlassian $1286.5M $1238.0M 3.9% $1349.0M $1311.1M 2.99 Dvnatrace $436.0M $426.5M 2.2% $434.5M $426.5M 1.99',\n",
       " 'EV/NTM Ev/2026 EV/NTM NTMRev Gross Operating FCF % in To Company Rev Rev FCF Growth Margin Margin Margin OverL 1 Palantir 54.4x 42.4x 137x 25% 81% 14% 37% 99% 2 Cloudflare 23.9x 17.9x 197x 25% 78% (10%) 11% 1009 3 CrowdStrike 20.9x 16.2x 80x 20% 75% (0%) 30% 1009 4 Samsara 19.7x 152K 199x 23% 76% (19%) 2% 1009 5 ServiceNow 15.5x 12.9x 49x 20% 79% 12% 31% 1009 6 Datadog 15.0x 11.7x 51x 21% 81% 3% 29% 1009 7 Shopify 14.9x 11.8x 75x 22% 51% 11% 17% 23% 8 Guidewire 14.4x 12.3x 75x 16% 61% (2%) 18% 34% 9 Snowflake 13.8x 10.7x 55x 22% 67% (39%) 23% 72% 10 HubSpot 13.4x 11.0x 76x 15% 85% (3%) 16% 41% Clouded Judgement @jaminball ALTIME)',\n",
       " '7 Day Share 30DayShare YTDShare Current Mark Company Price A Price A Price A Cap ($MM) 1 Agora 33% 45% 45% $552 2 Twilio 31% 37% 37% $22,754 3 Shopify 12% 12% 12% $153,937 4 Cloudflare 11% 27% 27% $47,275 5 Zoom 9% T% T% $26,864 6 GitLab 9% 23% 23% $11,260 7 Samsara 8% 19% 19% $29,177 8 Digital Ocean 8% 22% 22% $3,848 9 Klaviyo 8% 15% 15% $12,894 10 Domo 7% 17% 17% $323 Clouded Judgement @jaminball ALTIMETE',\n",
       " 'Multiple L™ Gross Operating Operating GM Adj. Market Cap EV L™ NnT™ 2026 L™ NT L™ NTM Revenue _ Margin Margin LT™ _WNTM, LT™ NT S&M R&O G&A SBC Expansion Payback Curront % Week % 30 $184,955 $180,739 68.3x 54.4x 42.4x 184x 137x 25% 25% $2,646 81% 14% 37% 38% 62% 63% 30% 17% 20% 20% NA NA $81 3% 7™% Cloudflare $47,275 $46,902 29.8x 23,9x 17.9% 276x 197x W% 25% $1,572 78% (10%) 11% 11% 41% 3% 46% 25% 17% 20% 110% 22 Months $137 11% 27% CrowdStrike $97,760 $94,330 25.2x 20,9x 16,2x 85x 80x 31% 20% $3,740 75% (0%) 30% 26% 61% 47% 38% 26% 12% 21% 115% 37 Months $397 5% 16% Samsara $29,177 $28,591 24.2x 19.7x 15.2x 1505x 199x 39% 23% $1,179 76% (19%) 2% 10% 41% 33% 49% 25% 20% 23% 115% 28 Months $52 8% 19% ServiceNow $208,627 $205,143 18.7x 15,5x 12.9x 60x 49x 22% 20% $10,984 79% 12% 31% 32% 54% 52% 35% 23% 9% 16% NA 24 Months $1,013 (11%) (4%) Datadog $48,381 $46,152 18,2x 15,0x 11,7x 63x 51x 26% 21% $2,536 81% 3% 29% 28% 55% 49% 28% 4% 7% 21% 115% 16 Months $143 2% (0%) $153,937 $150,183 18.3x 14.9x 11.8x 105x 75x 23% 22% $8,212 51% 11% 17% 18% 41% 41% 17% 16% 5% 5% NA NA $119 12% 12% Guidewire $17,667 $17,312 16.7x 14.4x 12.3x 93x 75x 13% 16% $1,036 61% (2%) 18% 19% 31% 35% 20% 27% 17% 14% NA NA $212 3% 25% Snowfiake $59,184 $57,627 16,9x 13,8x 10,7x 73x 55x Wes 22% $3,414 67% (39%) 23% 25% 54% 47% 47% 48% 11% 40% 127% 26 Months $179 2% 16% $39,853 $38,673 15.4x 13.4x 11.0x 99x 76x 22% 15% $2,506 85% (3%) 16% 17% 37% 32% 47% 23% 12% 19% NA 33 Months $772 5% 11% $13,714 $13,410 16.7x 12.9% 9.7x NM 435x 37% 20% $803 69% (137%) (5%) 3% 32% 32% 104% 64% 39% 103% 120% 32 Months $74 5% 13% ServiceTitan $9,025 $10,833 15,0x 12.6% \" NA 5S4x 47x 24% 18% $724 64% (22%) (2%) 3% 22% 21% 32% 33% 20% 14% 110% 43 Months $100 0% (2%) Atto $122,985 $116,500 14.1x 12.3x 10.3x 36x 32x 15% 14% $8,288 74% 9% 37% 39% 52% 53% 34% 23% 8% 13% NA NA $187 0% 3% Atiassian $69,552 $68,327 14.2x 12.3x 10.2 52x 44x 23% 16% $4,795 82% (3%) 29% 28% 52% 44% 20% 50% 13% 25% NA 11 Months $267 O% 10% OneStream $6,992 $6,645 14,5x 11,9x 11.5% \" NA 116x 31% 21% S460 NA NA NA 10% NA 31% NA NA NA NA NA 41 Months $30 (2%) 5% $11,260 $10,389 14.6x 11.7x 9.0x NM 71x 32% 25% $712 89% (22%) (15%) 16% 17% 41% 53% 32% 26% 26% 124% 27 Months $69 9% 23% $38,488 $33,497 12.6x 11.3x 9.7x 31x 28x 16% 12% $2,656 74% 24% 42% 41% 58% 52% 15% 26% 9% 16% NA 21 Months $237 6% 13% $12,894 $12,110 13.9% 11,1x 8.6x 94x 79x 35% 26% $869 78% (10%) 8% 13% 44% 39% 42% 22% 15% 16% 110% 29 Months S47 8% 15% $31,122 $29,651 12.9x 10.8 8.5x 45x 45x 31% 20% $2,299 78% (5%) 28% 24% 59% 44% 50% 24% 9% 24% 114% 33 Months $203 5% 12% Autodesk $66,639 $67.240 11.3x 10, tx 8.8x 51x 36x 12% 12% $5,961 91% 22% 22% 28% 34% 40% 33% 24% 11% 11% 105% NA $310 3% 5% AppFolio $9,216 $8,926 11,.2x 9.7x 8.3x 52x 37x 28% 16% $794 65% 18% 23% 26% 51% 42% 13% 20% 11% 8% NA NM $254 (3%) 3% Monday.com $11,833 $10,536 11,6x 9.2x 7.0x 38x 32x ws 27% $907 89% (4%) 31% 28% 65% 54% 57% 21% 15% 13% 111% 29 Months $242 (3%) 3% $11,908 $11,173 10.1x 9.0x 77x 72x 64x 24% 12% $1,110 82% (10%) 14% 14% 38% 26% 46% 27% 19% 16% NA 41 Months $80 2% ™ Dynatrace $17,293 $16,361 10.0x 8.9x 7.6x 40x 35x 20% 13% $1,634 81% 10% 25% 25% 45% 38% 36% 22% 12% 16% 111% 70 Months $58 5% 6% Mongod8 $20,187 $19,087 10,0x 8.6x 69x 129x 1140x 21% 16% $1,916 74% (14%) 8% 8% 29% 23% 45% 31% 12% 26% 120% 18 Months $271 4% 16% $194,144 $192,314 8.9x 8.2x 74x 24x 21x 11% 9% $21,505 89% 3% 37% 39% 47% 49% 27% 18% 7% 9% NA NA $446 2% o% Confluent $9,772 $9,026 9.9x 8,2x 64x NM 150x 25% 20% $916 73% (43%) (1%) 5% 24% 26% 57% 43% 17% 42% 117% 35 Months $30 4% 6% Salesforce $328,796 $328,217 8.8x 8.1x 7.3x 28x 26x 10% 9% $37,189 77% 20% 32% 32% 41% 40% 36% 14% 7% 8% NA 108 Months $344 3% 3% $4,137 $3,476 10.0x 8.1x 64x NM NM 22% 24% $347 60% (90%) (17%) (9%) 5% 15% 66% 60% 24% 64% NA 33 Months $32 (6%) (7%) $5,901 $6,035 8.9x 8.0x 7.0x 61x 53x 11% 1% $676 50% (7%) 15% 15% 26% 26% 16% 21% 18% 13% NA 78 Months $98 3% (3%) Hashicorp $7,002 $5,738 8.8x 77« 6.5x 100x 66x 16% 14% $655 82% (29%) 9% 12% 25% 25% 55% 34% 23% 25% 109% 37 Months $34 0% 0% $3,945 $3,493 8.5x 7.3% 6.0x 38x 36x 24% 17% $410 78% (20%) 22% 20% 47% 37% 44% 36% 17% 29% 117% 30 Months $35 2% 20% $5,113 $4,587 7.7x 7.2x 6.6x 21x 19x 10% 8% $593 82% W% 37% 37% 47% 45% 21% 19% 11% 13% NA NA $140 (2%) (0%) SentinelOne $7,751 $6,901 9.0x 71x 5.5x 1372x 103x 4% 26% $770 74% (43%) 1% 7% 35% 33% 60% 32% 24% 32% NA 36 Months $24 4% 9% Workday $69,179 $65,384 8.0x 71x 6.0x 31x 27x 17% 13% $8,157 76% 5% 26% 26% 43% 39% 29% 32% 10% 18% NA 43 Months $260 2% 1% $11,462 $10,853 7.9x 7.0x 6.0x S4x 50x 19% 12% $1,376 74% (8%) 15% 14% 33% 26% 43% 26% 12% 18% 112% 37 Months $111 6% 12% Paylocity $11,231 $10,831 7.7K 6.9x 6.0x 31x 30x 9% 12% $1,403 69% 19% 25% 22% 34% 34% 24% 13% 13% 10% NA NA $201 (1%) 1% Wix.com $13,288 $13,293 7.8x 6.8x 5.8x 31x 23x 13% 15% $1,704 68% 4% 25% 29% 38% 44% 25% 29% 10% 14% NA NA $242 2% 13% $5,489 $5,503 7.8x 6.7x 5.6x 82x 49x 16% 16% $705 77% (10%) 10% 13% 26% W% 47% 26% 14% 14% 111% 32 Months $99 (1%) (9%) $4,746 $4,345 7.7x 6.6x S.A4x 4893x 97x 28% 17% $564 68% (24%) 0% 7% 28% 24% 49% 23% 20% 20% 113% 47 Months S46 1% 10% $4,002 $3,819 7.3K 64x 5.4x 53x 35x 13% 14% $523 60% (3%) 14% 18% 27% 33% 23% 24% 16% 13% NA 25 Months $35 6% 3%',\n",
       " 'Valuation Rev Multiplo FCF Multiple Rev Growth LIM Gross Operating FCF Margin Rule of 40 LTM Operating Expenses % Rev Net GM Adj. Share Price Performar Market Cap EV LT™ NT 2026 LT™™ NTM LT NTM Revenue _ Margin Margin LT NTM L™ NT™ S&M R&D G&A SBC Expansion Payback Current % Week % 30 Bil.com $9,948 $9,450 7.0x 6.3x §.2x 38x 40x 19% 12% $1,344 82% (7%) 20% 15% 39% 27% 36% 24% 19% 18% 92% 33 Months $96 4% 13% Freshworks $5,806 $4,782 7.0x 6.1x $.2x 34x 24x 20% 15% S686 84% (23%) 20% 25% 41% 40% 57% 23% 26% 32% 107% 3 Months $19 3% 15% Asana $4.885 $4,699 6.6x 6.0x 5.2x NM 155x 12% 10% $707 89% (38%) (4%) 4% 8% 14% 60% 48% 20% 30% 96% 78 Months $21 5% 5% DocuSign $19,364 $18,552 64x 6.0x 5.5x 21x 21x 8% 6% $2,913 79% 6% 31% 29% 38% 35% 40% 20% 13% 21% 100% 63 Months $96 6% 7% Unity $9,269 $10,464 5.3x 6.0x S.4x 43x 37x (3.1%) (11%) = $1,965 68% (45%) 12% 16% 9% 5% 40% 49% 24% 34% NA NA $23 (1%) 2% Digital Ocean $3.848 $5,033 6.7x 5.9x 5.0x 53x 34x 12% 12% $757 60% 9% 13% 17% 25% 29% 10% 18% 23% 12% 97% 24 Months $42 8% 22% BlackLine $3,964 $4,065 6.4x 5.9x 5.2x 25x 24x 11% 0% $640 75% 4% 25% 24% 37% 32% 38% 15% 18% 13% 105% 46 Months $63 5% 4% Paycom $11,836 $11,591 64x 5.8x $.2x 37x 32x 12% 10% $1,824 82% 32% 17% 18% 29% 28% 24% 13% 9% (1%) NA NA $205 0% 0% Okta $16,066 $14,880 5.9x SAx 4.9x 24x 22x 17% 8% $2,533 76% (5%) 24% 25% 41% 33% 39% 25% 18% 23% 108% 49 Months $94 6% 19% Tenable $5,192 $5,051 5.8x 5.3x 4.7x 31x 20x 14% 0% $878 78% (3%) 19% 26% 33% 35% 46% 20% 14% 18% NA 65 Months $43 (1%) 10% Box $4,775 $5,331 5.0x 4.7% 4.3% 18x 17x 4% 6% $1,073 78% 8% 28% 28% 32% 34% 34% 24% 12% 19% 102% 61 Months $33 6% 5% Twillo $22,754 $21,182 4.9% 45x ax 29x 28x 6% 9% $4,339 51% (2%) 17% 16% 23% 24% 20% 23% 10% 14% 105% 24 Months $148 31% 37% Dropbox $9,952 $11,084 44x 43x 43x 12x 12x 3% 0% $2,540 82% 20% 35% 37% 38% 38% 18% 35% 9% 14% NA 273 Months $32 3% 7% UiPath $8.013 $6,491 46x 4.2x 3.7x 21x 18x 17% o% $1,411 83% (13%) 22% 23% 38% 32% 53% 26% 17% 25% 113% 51 Months $15 6% 15% Appian $2,599 $2,778 4.7% 4.2« 3.7x NM 162x 13% 11% $596 75% (14%) (3%) 3% 10% 14% 40% 26% 24% 7% 117% 26 Months $35 4% 7% Zoominfo $3,595 $4,882 4.0x 41x 4.0x 15x 12x (0%) (3%) $1,222 85% 12% 27% 33% 27% 30% 32% 15% 24% 12% 85% 30 Months. $10 5% (0%) Sprout Social $1,907 $1,832 4.7% 4.1x 3.4x 91x 36x 27% 14% $392 77% (17%) 5% 11% 32% 25% 47% 25% 22% 21% NA 65 Months $33 (1%) 8% Zoom $26,864 $19.229 4.2x 4.0x 3.9x Vix 12x 3% 3% $4,628 76% 16% 37% 33% 40% 35% 31% 18% 10% 21% 98% 95 Months $87 9% 7% Amplitude $1,523 $1,208 4.1% 3.8x 3.4x 103x 60x 8% 8% $293 74% (32%) 4% 6% 12% 14% 56% 29% 21% 30% 98% 88 Months $12 6% 13% Couchbase $910 $774 3.8x 34x 2.8x NM NM 19% 11% $205 88% (39%) (15%) (2%) 4% 9% 69% 34% 24% 29% 115% 79 Months $17 (1%) 19% PagerDuty $1,663 $1,597 3.5x 3.2x 2.8x 16x 20x 9% 9% $457 82% (18%) 22% 16% 30% 25% 44% 31% 25% 28% 107% 61 Months $18 (1%) 1% Jam{ $1,978 $2,152 3.5x 3.2« 2.7% 71x 15x 14% 11% $615 77% (13%) 5% 21% 19% 32% 41% 22% 22% 15% 106% 72 Months $15 4% 10% Zvora $1,629 $1,479 3.3x 3.1x 2.8x 21x 18x 7% 6% $453 67% (9%) 16% 18% 23% 24% 33% 23% 20% 20% 103% 85 Months $10 0% 1% Fived $3,065 $3,336 3.3x 3.0x 2.7x 49x 24x 14% 10% $1,002 53% (7%) 7% 12% 21% 23% 31% 16% 13% 18% NA 39 Months S41 4% 0% Fastly $1,505 $1,611 3.0x 2.9x 2.6x NM NM 11% 3% $841 55% (30%) (6%) (0%) 5% 3% 36% 27% 22% 22% 105% 60 Months $11 3% 14% Olo $1,241 $906 3.3x 2.9% 24x 40x 35x 26% 16% $272 56% (12%) 8% 8% 35% 23% 19% 25% 24% 16% 120% 53 Months 37 0% (3%) Sprinkle $2,259 $1,833 2.3x 2.3« 2.1x 26x 26x 12% 3% $788 73% 5% 9% 9% 21% 12% 41% 12% 17% 1% 107% 122 Months $9 0% 5% Agora $552 $272 2.0x 1.9x 1.7x nM” Ox (7%) 4% $135 63% (42%) (34%) 10% (41%) 14% 20% 61% 25% 19% 94% NM $6 33% 45% RingCentral $3,203 $4,771 2.0x 1.9x 17x 12x 10x 9% 7% $2,357 70% (2%) 17% 19% 27% 26% 46% 14% 13% 16% 99% 90 Months $36 3% 2% Yext $849 $847 2.1x 18x 1.8% 22x 16x 1% 12% $409 78% (5%) 9% 12% 10% 24% 42% 18% 23% 12% 91% 11 Months $7 5% 5% Kattura $343 $314 1.8x 17x 1.7x 38x 25x 2% 1% $178 65% (16%) 5% 7™% 6% 8% 27% 28% 4% 16% 101% 12 Months $2 2% 4% Riskified $887 $554 17x 17x 14x 16x 12x 9% 4% $318 54% (16%) 11% 13% 20% 17% 28% 22% 20% 19% NA NA $s 1% 10% CS Disco $309 $240 1.7% 1.6x 1.5x NM NM 6% 4% $144 75% (30%) (8%) (9%) (1%) (6%) 42% 34% 29% 15% NA 239 Months $5 4% 3% BigCommerce $489 $543 1.6x 1.6x 14x 23x 19x 11% 4% $330 77% (10%) 7% 8% 18% 13% 41% 24% 18% 1% NA 271 Months LS] 0% 2% Domo $323 $411 13x 1.3« 1.3x NM NM 0% (2%) $318 75% (20%) (7%) (2%) (7%) (3%) 49% 27% 18% 19% NA 366 Months $8 7% 17% 8x8 $384 $689 1.0x 0.9x 1.0x 14x ox (2%) 1% $720 68% (1%) 7% 10% 4% 11% 37% 18% 14% 7% NA 130 Months $3 7% 10% 74% NA NM 37 On24 $289 $103 O.7x 0.7x 0.7x 589x 208 (12%) (7%) = S151 (34%) 0% 0% (12%) (7%) 53% 24% 31% 31%']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20.0x : 4 15.0x 10.0x 7.8X € 5.0x , 0.0x ( eS © £ SS SP Po PP OD PD oP PP» - x yr Ye fo) we Y yy a we ys Yy yw o we ” Median —— = LT Pre Covid Average — 10 Year Treasury Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIMET',\n",
       " '90.0x 80.0x 70.0x 60.0x 50.0x 40.0x 30.0x 20.0x 10.0x 0.0x Oo 2 © © BS YS 8 PMP YP SP PD DD DD PDP PD DP PF we fe * € S SF Ss KS os S si ——= Median —— Top 5 Median Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIMET',\n",
       " '40.0x 35.0x 30.0x 25.0x 20.0x 15.0x 10.0x 5.0x 4 0.0x © © ye OS se © Xe) AN “4 C2) 2 2 S) o y vy 1y > > b & Pome » os Pa a $ Ry ri WV e Rie Ry Pag ~Y or fw fs) S < g ys x » er PS SS ° RX ¥ =——=High Growth Median == Vid Growth Median —— Low Growth Median Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIMET',\n",
       " 'Sax 15x 15x 14x 13x aa 13x 12x 1x ux Clouded Judgement @jaminbalt',\n",
       " '1.0x 0.9x 0.8x 4 0.7x 0.6x 0. 0.5x 0.4x 0.3x 0.2x 0.1x 0.0x 2 ew © SS Se Pp PS PO DY OS e yy o we sy ee RS ss Rs w == Median EV / Rev / Growth ——LT Average —— 10 Year Treasury Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIME1',\n",
       " 'Growth Adjusted EV / NTM Revenue (EV / NTM Rev / NTM Growth) 25x 2ax 2.0% 15 1.0% sx ’ Median: 0.6 osx F lil TT 0.8K 0.8% ayy 5.0% osx, 03x . TTT 228g ° 33 22388 ° 32 Beze° 2eaeE28 Peet ees sersese oeree g 2*s. Clouded Judgement @jaminball mii',\n",
       " '70.0x 60.0x 50.0x 40.0x ac 30.0x 20.0x 10.0x -Ox 1/1/15 1/1/16 1/1/17 1/1/18 1/1/19 1/1/20 1/1/21 1/1/22 1/1/23 1/1/24 1/1/25 FCF Positive & <100x Multiple Median ——LT Average',\n",
       " 'EV / NTM FCF a a 39% P3652 ye 2% 5, 2X 22K, 21x 20% 32x 30x | 28.28% we a meee Clouded Judgement @jaminbalt ALT',\n",
       " 'a CRWD ae in\" o ‘ , . 20x e +” y= 60.879x + 0.2737 \\\\, ‘ R? =0.3849 rf cowne PW on06 SHOP Sy Ps vues e a - man @ ene 10x CaM nose < S00 wg © mor am e 9 © sor rem FLY PL pp p 00 Py 200 & FAME very iy oo 0% 5% 10% 15% 20% 25% 30% Clouded Judgement @jaminball ALTIM']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements2=partition_pdf(\n",
    "    filename=\"data/2005.11401v4.pdf\",                  # mandatory\n",
    "    strategy=\"hi_res\",                                 # mandatory to use ``hi_res`` strategy\n",
    "    extract_images_in_pdf=True,                       # mandatory to set as ``True``\n",
    "    extract_image_block_types=[\"Image\",\"Table\"],          # optional\n",
    "    extract_image_block_to_payload=False,                  # optional\n",
    "    extract_image_block_output_dir=\"extracted_data2\",  # optional - only works when ``extract_image_block_to_payload=False``\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Text at 0x1d6ae9eb1f0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae9e97e0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae9ea050>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae9e88e0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae9e9ff0>,\n",
       " <unstructured.documents.elements.Header at 0x1d6b9eba710>,\n",
       " <unstructured.documents.elements.Text at 0x1d6ae9e86d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e8730>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e90c0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e9180>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e8280>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9eaf80>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b9ebb910>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9eb8d60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9ebb5b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9eb9d80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9eba3b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9eb8af0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b9eb8e20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9ebb940>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7a8070>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9690>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ae7a8ac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9660>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a90f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9300>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9ae0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a8280>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7a9cc0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a96c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a8e80>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6ae7a9f90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9420>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a8250>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7a9c60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a97e0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9eb460>,\n",
       " <unstructured.documents.elements.Formula at 0x1d6ae7a81c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9a50>,\n",
       " <unstructured.documents.elements.Formula at 0x1d6ae7a9a20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9d20>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae7a8a60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae7a9d80>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e8b80>,\n",
       " <unstructured.documents.elements.Formula at 0x1d6ae7a8be0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9ea5f0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9eab60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9eb160>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e9f30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9eacb0>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6ae9e9cc0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca48b20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9eb190>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9eae00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca48490>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9ea950>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9ebb80>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e9300>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9e8430>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9eabc0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9ea7a0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9ebbb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9e8250>,\n",
       " <unstructured.documents.elements.Text at 0x1d6aca48190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9e8070>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e80d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9ebd30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9e9000>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9e8b20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9eb6d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9eb5b0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ae9ebca0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9e9420>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ae9e84c0>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6ae9ebfd0>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x1d6ae9ea9b0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca48610>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca49330>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca49120>,\n",
       " <unstructured.documents.elements.Table at 0x1d6b8d5aa70>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4aad0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6aca4a230>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d59b70>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8d5ab00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5a980>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8d5a920>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5b9a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5b850>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8d59bd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5a620>,\n",
       " <unstructured.documents.elements.Text at 0x1d6aca4aa70>,\n",
       " <unstructured.documents.elements.Image at 0x1d6b8d5a170>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5a830>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5ac80>,\n",
       " <unstructured.documents.elements.Table at 0x1d6b8d5bb20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5bc10>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8d5a4a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d59ab0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5a800>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5bb50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5a290>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6b8d58100>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8d5bc40>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca48040>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4b6a0>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ad3ab430>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x1d6ad3abfd0>,\n",
       " <unstructured.documents.elements.Table at 0x1d6ad3a8fd0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca49240>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4acb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad3a9c90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad3a84f0>,\n",
       " <unstructured.documents.elements.Image at 0x1d6ad3a9bd0>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x1d6ad3a9f90>,\n",
       " <unstructured.documents.elements.Title at 0x1d6ad3aa470>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad3aa8f0>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6b8c69030>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6a9e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6b520>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6b550>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6af50>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8c69ae0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6aaa0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6aca48160>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8c6a080>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6b970>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6afe0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8c69d50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b8c6ac50>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b8c6a8c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6bfa0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c69ff0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6a590>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6b880>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6bd60>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6a680>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6b8c68640>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6a380>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6a3b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c69d20>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6a560>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c69f30>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6bd00>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6ac20>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c686a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6b8c6a0b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad144460>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1476d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146da0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146aa0>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6ad1470d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad144310>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1446d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad145570>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad144a30>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad147520>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1458d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad145660>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146260>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad145e10>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146530>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1471c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146c20>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6ad146c80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad147ee0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146020>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1455a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146d10>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad145a20>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146230>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1457b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1459c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1450c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad144250>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146290>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad145480>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1474c0>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6ad146410>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6ad1474f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad147d90>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1465f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad145de0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad147220>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad147640>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad56a590>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146350>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad1451e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ad146b30>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84fcd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84d210>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84dcc0>,\n",
       " <unstructured.documents.elements.Text at 0x1d6aca481c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84e320>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84da80>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84ec80>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84ebf0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84e0e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84d1b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84ee00>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84ffa0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6ac84fca0>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6aca49ea0>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6aca4a470>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6aca48370>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6aca49630>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d6aca4ba00>,\n",
       " <unstructured.documents.elements.Header at 0x1d6aca489d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca48a90>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca49570>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca49450>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca48f40>,\n",
       " <unstructured.documents.elements.Image at 0x1d6aca4a1a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4a050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4a140>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4ad10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4a5f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4a440>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6aca4ab90>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4bac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4b8e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4b8b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4b490>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4b2e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4b2b0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4a920>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4ae00>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca4a2f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca49a80>,\n",
       " <unstructured.documents.elements.Footer at 0x1d6aca49c00>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x1d6aca49e10>,\n",
       " <unstructured.documents.elements.Table at 0x1d6aca49b70>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca498d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6aca49840>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6aca4a7d0>,\n",
       " <unstructured.documents.elements.Title at 0x1d6b9e1ca30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d6b9e1c820>,\n",
       " <unstructured.documents.elements.Header at 0x1d6b9e1d810>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            Table.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - - /50.1 37.4 /60.5 44.7 - - Model B-1 QB-1 R-L B-1 Label Acc. Open REALM [20] 40.4 - / - 40.7 46.8 SotA - - 49.8* 49.9* 76.8 Book DPR [26] 41.5 57.9/ - 41.1 50.6 BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5',\n",
       " 'Task Input Model Generation MS- deﬁne middle ear BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. MARCO what currency BART The currency needed in Scotland is Pound sterling. needed in RAG-T Pound is the currency needed in Scotland. scotland RAG-S The currency needed in Scotland is the pound sterling. BART ?This state has the largest number of counties in the U.S. Jeopardy Washington RAG-T It’s the only U.S. state named for a U.S. president Question RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park Gener -ation The Divine Comedy BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"',\n",
       " 'Factuality Speciﬁcity MSMARCO Jeopardy QGen BART better RAG better Both good Both poor No majority 7.1% 42.7% 11.7% 17.7% 20.8% 16.8% 37.4% 11.8% 6.9% 20.1% Gold BART RAG-Token RAG-Seq. 89.6% 70.7% 77.8% 83.5% 90.0% 32.4% 46.8% 53.8%',\n",
       " 'Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 Exact Match B-1 QB-1 R-L B-1 RAG-Token-BM25 RAG-Sequence-BM25 29.7 31.8 41.5 44.1 32.1 36.6 33.1 33.8 17.5 11.1 22.3 19.5 55.5 56.5 48.4 46.9 75.1 91.6 RAG-Token-Frozen RAG-Sequence-Frozen 37.8 41.2 50.1 52.1 37.1 41.8 51.1 52.6 16.7 11.8 21.7 19.6 55.9 56.7 49.4 47.3 72.9 89.4 RAG-Token RAG-Sequence 43.5 44.0 54.8 55.8 46.5 44.9 51.9 53.4 17.9 15.3 22.6 21.5 56.2 57.2 49.4 47.5 74.5 90.6',\n",
       " 'Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generation 97392 13714 26849 MS-MARCO 153726 12468 101093* FEVER-3-way 145450 10000 10000 FEVER-2-way 96966 6666 6666']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "            Text.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Patrick Lewis't, Ethan Perez*,\",\n",
       " 'Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,',\n",
       " 'Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†',\n",
       " 'tRacebook AI Research; ‘University College London; *New York University;',\n",
       " 'plewis@fb.com',\n",
       " 'Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
       " 'Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,',\n",
       " 'Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.',\n",
       " 'but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.',\n",
       " 'We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.',\n",
       " 'There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.',\n",
       " 'Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera- tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1',\n",
       " 'We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized',\n",
       " '1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/',\n",
       " 'by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z.',\n",
       " 'To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.',\n",
       " 'RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,',\n",
       " 'RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:',\n",
       " 'Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.',\n",
       " 'The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:',\n",
       " 'where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.',\n",
       " 'The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.',\n",
       " 'We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we',\n",
       " 'minimize the negative marginal log-likelihood of each target, y)',\n",
       " 'j − log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.',\n",
       " 'At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).',\n",
       " 'RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: pj (yi|z, yii—1) = De zetop-k(p(-l2)) Pn (Zi|@)po(Yil@, Zi, Y14—-1) To decode, we can plug Po(yi |x, y1i—1) into a standard beam decoder.',\n",
       " 'RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”',\n",
       " 'We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.',\n",
       " 'Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.',\n",
       " 'RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat',\n",
       " 'MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.',\n",
       " 'To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.',\n",
       " 'We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good.',\n",
       " 'FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.',\n",
       " 'Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.',\n",
       " 'There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading',\n",
       " 'to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.',\n",
       " 'As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).',\n",
       " 'Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model.',\n",
       " 'Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.',\n",
       " 'Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.',\n",
       " 'Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem- ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".',\n",
       " 'Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.',\n",
       " 'For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.',\n",
       " 'Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing any diversity-promoting decoding.',\n",
       " 'Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.',\n",
       " 'We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.',\n",
       " 'Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed',\n",
       " 'Table 4: Human assessments for the Jeopardy Question Generation Task.',\n",
       " 'between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.',\n",
       " 'Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in performance between them. We have the ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.',\n",
       " 'Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.',\n",
       " 'General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models.',\n",
       " 'Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be ﬁne-tuned for strong performance on a variety of tasks.',\n",
       " 'Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].',\n",
       " 'Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.',\n",
       " 'In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.',\n",
       " 'This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.',\n",
       " 'With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against misleading content and automated spam/phishing.',\n",
       " 'The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.',\n",
       " 'for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/ anthology/P19-1612.',\n",
       " 'approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf.',\n",
       " 'For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.',\n",
       " 'Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".',\n",
       " 'Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instruc- tions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.',\n",
       " 'We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision ﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼ 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/ blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/',\n",
       " '2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers',\n",
       " 'For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur in top 1000 documents for the query.',\n",
       " 'CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres- sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.',\n",
       " 'TriviaQA Evaluation setups The open-domain QA community customarily uses public develop- ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being simpler to answer from Wikipedia.',\n",
       " 'For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The ﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.',\n",
       " 'We experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document mechanisms may not be necessary for RAG.',\n",
       " 'Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable',\n",
       " 'parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating point precision to manage memory and disk footprints.',\n",
       " 'In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.',\n",
       " 'The number of training, development and test datapoints in each of our datasets is shown in Table 7.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "            Image.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListItem=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "            Image.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ListItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ieee ee ee ee ee ee eee End-to-End Backprop through q and pe Define \"middle ear\" (x) Question Answering: Question Query Barack Obama was Document Generator pg Index (Parametric) Retriever py (Non-Parametric) The middle ear includes the tympanic cavity and the three ossicles. (y) Question Answering ‘Answer Generation born in Hawaii. (x) supports (y) a(x) Fact Verification: Label Generation Fact Verification: Fact Query Margin- alize The Divine This 14th century work Comedy (x) is divided into 3 Jeopardy Question Generation: Answer Query sections: \"purgatorio\" & \"Paradiso\" @) \"Inferno\", Question Generation',\n",
       " 'Document 1: his works are considered classics of American Doc 1 + | | literature ... His wartime experiences formed the basis for his novel poe 2 - | | “A Farewell to Arms” (1929) ... Doc 3 + Document 2: ... artists of the 1920s “Lost Generation” expatriate Doe 4+ community. His debut novel, \"The Sun Also Rises”, was published °° in 1926. Doc 5 + RS ee SS \\' errs es BS s ¥ £ ‘ eser & S g@e',\n",
       " '80 OO s g 3 3 RAG TORRE 2 2 So RAG TOK BA & 2 RAG Tok ve RAG Seq RL. I 2s ; woe racseq | oo RAGSey Bt 4 — ractk | Gg ff 7 —o- Fixed DPR | 2 == RAGSeq | Z 40 < BM25 Bag] -. 10 20 30 20 30 K Retrieved Docs K Retrieved Docs 40 50 10 20 30 K Retrieved Docs 40 50',\n",
       " 'View full instructions Which sentence is more factually true? View tool guide Select an option Subject: Hemingway a Note: Some questions are Sentence Ais more 1 control questions. We require Sentence A : \"The Sun Also Rises\" is a novel by this author of \"A true good accuracy on our control Farewell to Arms\" Sentence Bismore 2 questions to accept true responses. Sentence B : This author of \"The Sun Also Rises\" was born in Both sentences are 8 Havana, Cuba, the son of Spanish immigrants ‘true Indicate which one of the P 3 following sentences is more Both sentences are factually true with respect to completely untrue the subject. Using the internet to check whether the sentences are true is encouraged.',\n",
       " '[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http: //arxiv.org/abs/1611.09268. arXiv: 1611.09268.',\n",
       " '[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20.',\n",
       " '[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ D13-1160.',\n",
       " '[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod- ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.',\n",
       " '[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171.',\n",
       " '[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.',\n",
       " '[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre- hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.',\n",
       " '[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.',\n",
       " '[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz- ard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.',\n",
       " '[10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.',\n",
       " '[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/ P18-1082.',\n",
       " '[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/ anthology/P19-1346.',\n",
       " '[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id= H1gx1CNKPH.',\n",
       " '[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.',\n",
       " '[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/ AAAI/AAAI18/paper/view/16710.',\n",
       " '[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.',\n",
       " '[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artiﬁcial Intelligence, 2018. URL https: //www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.',\n",
       " '[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.',\n",
       " '[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.',\n",
       " '[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https: //arxiv.org/abs/2002.08909.',\n",
       " '[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed- itors, Advances in Neural Information Processing Systems 31, pages 10052– 10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs. pdf.',\n",
       " '[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve- edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa- tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/ anthology/2020.acl-main.228.',\n",
       " '[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.',\n",
       " '[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.',\n",
       " '[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack- augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam- bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/ 5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.',\n",
       " '[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.',\n",
       " '[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza- tion through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.',\n",
       " '[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.',\n",
       " '[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken- ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques- tion Answering Research. Transactions of the Association of Computational Lin- guistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/ natural-questions/main-1455-kwiatkowski.pdf.',\n",
       " '[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In- formation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http: //papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.',\n",
       " '[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association',\n",
       " '[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.',\n",
       " '[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/ N16-1014.',\n",
       " '[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.',\n",
       " '[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.',\n",
       " '[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=Hyg0vbWC-.',\n",
       " '[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.',\n",
       " '[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.',\n",
       " '[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the veriﬁability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https: //arxiv.org/abs/1911.03587.',\n",
       " '[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.',\n",
       " '[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit- ing background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus- sels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.',\n",
       " '[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/ anthology/D18-1429.',\n",
       " '[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic',\n",
       " '[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.',\n",
       " '[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb. org/anthology/N19-4009.',\n",
       " '[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.',\n",
       " '[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1250. URL https://www.aclweb.org/anthology/D19-1250.',\n",
       " '[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum? id=025X0zPfn.',\n",
       " '[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im- proving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/ language-unsupervised/language_understanding_paper.pdf.',\n",
       " '[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf.',\n",
       " '[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.',\n",
       " '[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/ 2002.08910.',\n",
       " '[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/ 1500000019. URL https://doi.org/10.1561/1500000019.',\n",
       " '[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.',\n",
       " '[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net- works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.',\n",
       " '[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074.',\n",
       " '[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.',\n",
       " '[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.',\n",
       " '[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17329.',\n",
       " '[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/ anthology/W18-5446.',\n",
       " '[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General- Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https:// arxiv.org/abs/1905.00537.',\n",
       " '[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/16712.',\n",
       " '[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re- ranking in open-domain question answering. In ICLR, 2018. URL https://openreview. net/forum?id=rJl3yM-Ab.',\n",
       " '[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.',\n",
       " '[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713.',\n",
       " '[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.',\n",
       " '[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi- supervised question answering. In Proceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem- ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253.',\n",
       " '[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
    "    Give a concise summary of the table that is well optimized for retrieval. Table:{element} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_TOKEN = os.environ[\"OPENAI_API_KEY\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = summarize_chain.batch(Text, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table contains information about two individuals, Patrick Lewis and Ethan Perez. However, the provided information is incomplete, making it impossible to provide a detailed summary.',\n",
       " 'The table contains information about various individuals, namely Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, and Heinrich Küttler†. The specific details about these individuals are not provided in the prompt.',\n",
       " 'The table lists the names of five individuals: Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. The symbols † and ‡ are used, possibly indicating affiliations or footnotes related to these individuals.',\n",
       " 'The table contains information about AI research at different institutions, specifically tRacebook, University College London, and New York University.',\n",
       " \"The table contains data related to the email address 'plewis@fb.com', possibly including information such as user details, communication records, or other associated data.\",\n",
       " 'The table discusses the limitations of large pre-trained language models in accessing and manipulating knowledge, especially in knowledge-intensive tasks. It introduces Retrieval-Augmented Generation (RAG) models that combine pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia. Two RAG formulations are compared, one that conditions on the same retrieved passages across the whole generated sequence, and another that uses different passages per token. The RAG models are evaluated on various NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models are found to generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
       " 'The table discusses the advantages and limitations of pre-trained neural language models. It highlights their ability to learn in-depth knowledge from data without any external memory, functioning as a parameterized implicit knowledge base. However, these models have limitations such as difficulty in expanding or revising their memory, providing insight into their predictions, and the potential to produce \"hallucinations\". The table suggests that hybrid models, which combine parametric memory with non-parametric (retrieval-based) memories, can address these issues. It mentions REALM and ORQA, two recent models that combine masked language models with a differentiable retriever, as promising developments in this field.',\n",
       " 'The table provides an overview of a combined approach using a pre-trained retriever and a pre-trained seq2seq model. The process involves using a Query Encoder and Document Index for retrieval, and a Generator for sequence predictions. The Maximum Inner Product Search (MIPS) is used to find the top-K documents for a given query. The final prediction is made by treating the retrieved documents as a latent variable and marginalizing over sequence predictions.',\n",
       " 'The table discusses the exploration of open-domain extractive question answering in the context of hybrid parametric and non-parametric memory. It specifically focuses on the application of these concepts to sequence-to-sequence (seq2seq) models, referred to as the \"workhorse of NLP\".',\n",
       " 'The table discusses the concept of retrieval-augmented generation (RAG), a fine-tuning approach that combines pre-trained, parametric-memory generation models with a non-parametric memory. The parametric memory is a pre-trained seq2seq transformer, while the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. The model is trained end-to-end, with the retriever providing latent documents based on the input, and the seq2seq model generating the output based on these latent documents and the input. The latent documents are marginalized with a top-K approximation. RAG can be fine-tuned on any seq2seq task, with both the generator and retriever being jointly learned.',\n",
       " 'The table discusses various architectures designed to enhance systems with non-parametric memory, such as memory networks, stack-augmented networks, and memory layers. It contrasts these with a system where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge, emphasizing that this approach allows for knowledge access without additional training.',\n",
       " \"The table presents the results of combining parametric and non-parametric memory for knowledge-intensive tasks. The RAG models used in the study achieved state-of-the-art results on open Natural Questions, WebQuestions, and CuratedTrec, and outperformed recent approaches on TriviaQA. The models also performed well in knowledge-intensive generation tasks, producing more factual, specific, and diverse responses than a BART baseline in experiments with MS-MARCO and Jeopardy question generation. The models also achieved near state-of-the-art results in FEVER fact verification. The study also shows that the non-parametric memory can be updated to keep the models' knowledge current.\",\n",
       " 'The table discusses RAG models that utilize an input sequence to retrieve text documents and use them as context for generating a target sequence. It highlights two main components: a retriever that returns distributions over text passages based on a query, and a generator that is parameterized.',\n",
       " 'The table provides information about the open-source code for running experiments with RAG, which is part of the HuggingFace Transformers Library. The code can be accessed via a provided GitHub link. Additionally, an interactive demo of RAG models is available at a specified HuggingFace website.',\n",
       " 'The table describes a function by θ that generates a current token based on three parameters: the context of the previous i - 1 tokens (y1:i-1), the original input (x), and a retrieved passage (z).',\n",
       " 'The table discusses two models, RAG-Sequence and RAG-Token, used for training the retriever and generator end-to-end by treating the retrieved document as a latent variable. It explains how each model uses documents to predict target tokens differently. It also introduces the pη and pθ components and outlines the training and decoding procedure.',\n",
       " 'The table describes the RAG-Sequence model, which uses a single retrieved document to generate a complete sequence. It treats the document as a latent variable, which is marginalized to obtain the seq2seq probability through a top-K approximation. The model retrieves the top K documents and generates an output sequence probability for each, which are then marginalized.',\n",
       " 'The table describes the RAG-Token model, which allows the generator to select content from multiple documents when generating an answer. The top K documents are retrieved and the generator produces a distribution for the next output token for each document. This process is repeated for each subsequent output token. The model is defined formally within the table.',\n",
       " 'The table discusses the use of RAG (Retrieval-Augmented Generation) for sequence classification tasks. It highlights that the target class can be considered as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.',\n",
       " 'The table discusses the retrieval component pη(z|x) which is based on the Dense Passage Retrieval (DPR) method. DPR utilizes a bi-encoder architecture.',\n",
       " 'The table describes the process of document retrieval using a BERTBASE document encoder and a query encoder. It involves calculating the top-k documents with the highest prior probability, a problem known as Maximum Inner Product Search (MIPS). A pre-trained bi-encoder from DPR is used to initialize the retriever and build the document index. The retriever is trained to retrieve documents containing answers to TriviaQA and Natural Questions. The document index is referred to as the non-parametric memory.',\n",
       " 'The table discusses the use of BART-large, a pre-trained seq2seq transformer with 400M parameters, as a generator component in a model. The input x is combined with the retrieved content z by simple concatenation. BART, pre-trained using a denoising objective and various noising functions, has achieved state-of-the-art results on diverse generation tasks, outperforming similar-sized T5 models. The BART generator parameters are referred to as the parametric memory.',\n",
       " 'The table discusses the joint training of retriever and generator components without direct supervision. It explains the process using a fine-tuning training corpus of input/output pairs.',\n",
       " \"The table provides data on the process of minimizing the negative marginal log-likelihood of each target variable, denoted as 'y'.\",\n",
       " \"The table discusses the process of using stochastic gradient descent with Adam for the function j − log p(yj|xj). It mentions the costliness of updating the document encoder BERTd during training, as it necessitates periodic updates to the document index, similar to the REALM pre-training process. However, it suggests that this step isn't crucial for high performance. Instead, only the query encoder BERTq and the BART generator are fine-tuned, while the document encoder and index remain fixed.\",\n",
       " 'The table discusses the different methods required by RAG-Sequence and RAG-Token to approximate arg maxy p(y|x) during test time.',\n",
       " 'The table describes the RAG-Token model, a standard autoregressive seq2seq generator. It explains the transition probability formula and mentions that to decode, the given formula can be plugged into a standard beam decoder.',\n",
       " 'The table discusses the RAG-Sequence decoding procedures, \"Thorough Decoding\" and \"Fast Decoding\". Thorough Decoding involves running a beam search for each document, scoring each hypothesis, and estimating the probability of a hypothesis by running an additional forward pass for each document where the hypothesis does not appear in the beam. This process can be time-consuming for longer output sequences. Fast Decoding, on the other hand, is a more efficient method that approximates the probability of a hypothesis to zero if it was not generated during the beam search, eliminating the need for additional forward passes.',\n",
       " 'The table presents an experiment using RAG for various knowledge-intensive tasks, utilizing a single Wikipedia dump from December 2018 as a non-parametric knowledge source. The dump is divided into 21M documents, each containing 100-word chunks from Wikipedia articles. Document embeddings are computed using a document encoder and a single MIPS index is built using FAISS with a Hierarchical Navigable Small World approximation for quick retrieval. During training, the top k documents are retrieved for each query, with k being either 5 or 10 for training and set for test time using dev data.',\n",
       " 'The table compares the performance of the Retrieval-Augmented Generation (RAG) model with other models in open-domain question answering (QA) tasks. It uses four popular QA datasets: Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), and CuratedTrec (CT). The RAG model is trained by minimizing the negative log-likelihood of answers and is compared to extractive QA paradigms and Closed-Book QA approaches. The performance is evaluated based on Exact Match (EM) scores. For smaller datasets like CT and WQ, the models are initialized with the NQ RAG model. The TQA dataset is also evaluated on the TQA Wiki test set for comparison with the T5 model.',\n",
       " \"The table presents information about the use of RAG models in natural language generation (NLG) tasks, specifically the MSMARCO NLG task v2.1. It details the task's structure, which includes questions, ten gold passages per question retrieved from a search engine, and a full sentence answer annotated from these passages. The table also notes that only the questions and answers are used, not the supplied passages.\",\n",
       " 'The table discusses MSMARCO as an open-domain abstractive QA task, highlighting challenges such as certain questions that cannot be answered accurately without access to gold passages. It also mentions that some questions cannot be answered using only Wikipedia, suggesting the use of RAG for generating reasonable responses.',\n",
       " 'The table evaluates the generation abilities of RAG in a non-QA setting, focusing on open-domain question generation. It proposes a challenging task of generating Jeopardy questions, which are precise, factual statements, based on their answer entities. The uniqueness of Jeopardy format is highlighted, where an entity is guessed from a fact about it. An example is provided to illustrate this format.',\n",
       " 'The table presents a summary of a study using SearchQA splits with 100K train, 14K dev, and 27K test examples. A BART model is trained for comparison in this new task. The evaluation is done using the SQuAD-tuned Q-BLEU-1 metric, which is a variant of BLEU with a higher weight for matching entities. Two human evaluations are performed to assess generation factuality and specificity. Factuality is defined as the ability to corroborate a statement with trusted external sources, while specificity is defined as high mutual dependence between input and output. The evaluation method follows best practice and uses pairwise comparative evaluation. Evaluators are shown an answer and two generated questions, one from BART and one from RAG, and asked to choose one of four options to determine the better question.',\n",
       " \"The table discusses the FEVER task, which involves classifying a claim based on evidence from Wikipedia as either supported, refuted, or unverifiable. The task combines retrieval and entailment reasoning, testing the RAG models' classification abilities. The FEVER class labels are mapped to single output tokens and trained with claim-class pairs, without supervision on retrieved evidence. The table explores two variants: a standard 3-way classification (supports/refutes/not enough info) and a 2-way classification (supports/refutes), reporting label accuracy in both cases.\",\n",
       " 'The table presents the performance of the RAG model in comparison to other state-of-the-art models on four open-domain QA tasks. RAG sets a new standard, particularly on the T5-comparable split for TQA, by combining the benefits of both \"closed-book\" and \"open-book\" approaches. Unlike other models like REALM and T5+SSM, RAG achieves strong results without the need for costly, specialized pre-training. RAG\\'s retriever is initialized using DPR\\'s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. The model outperforms the DPR QA system, which uses a BERT-based \"cross-encoder\" to re-rank documents, demonstrating that neither a re-ranker nor extractive reader is necessary for top-tier performance.',\n",
       " 'The table discusses the benefits of generating answers instead of extracting them. It highlights that documents with indirect clues can contribute to generating the correct answer, a feature not possible with standard extractive methods.',\n",
       " \"The table discusses the effectiveness of RAG (Retrieval-Augmented Generation) in document marginalization. It highlights RAG's ability to generate correct answers even when the correct answer is not present in any retrieved document, achieving an accuracy of 11.8% in such cases for NQ (Natural Questions), compared to an extractive model which would score 0%.\",\n",
       " 'The table compares the performance of RAG-Sequence and BART on Open MS-MARCO NLG, with RAG-Sequence outperforming BART by 2.6 Bleu points and 2.6 Rouge-L points. It highlights the impressive performance of RAG models, despite limitations such as lack of access to gold passages and the inability to answer all questions from Wikipedia alone. The table also includes generated answers from the models, indicating that RAG models produce less hallucination and more factually correct text than BART. It also suggests that RAG generations are more diverse than BART generations.',\n",
       " 'The table compares the performance of RAG-Token, RAG-Sequence, and BART models on Jeopardy question generation. It shows RAG-Token outperforming the other two models, with human evaluators finding RAG more factual and specific than BART in most cases. The table also includes typical generations from each model.',\n",
       " 'The table discusses the performance of RAG-Token in generating responses for Jeopardy questions, which often contain two separate pieces of information. It highlights how the model uses both parametric and non-parametric memories to generate responses. The model\\'s parametric knowledge is sufficient to complete book titles without depending on specific documents. This is demonstrated through examples where BART, the baseline model, completes the partial decoding of book titles \"The Sun Also Rises\" and \"A Farewell to Arms\". The table suggests that the non-parametric component guides the generation process, drawing out specific knowledge stored in the parametric memory.',\n",
       " 'The table, Table 2, presents the performance results of RAG on FEVER in a 3-way classification. It shows that RAG scores are close to state-of-the-art models, with a difference of only 4.3%. These top models are complex pipeline systems with domain-specific architectures and require substantial engineering and intermediate retrieval supervision, which RAG does not need.',\n",
       " 'The table, Figure 2, presents the RAG-Token document posterior p(zi|x, yi, y−i) for each generated token with the input \"Hemingway\" in the context of Jeopardy generation using 5 retrieved documents. It highlights that the posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".',\n",
       " 'The table, titled \"Table 3: Examples from generation tasks\", compares the performance of RAG models in generating responses. It highlights that RAG models produce more specific and factually accurate responses. The table uses \\'?\\' to denote factually incorrect responses and \\'*\\' to indicate partially correct responses.',\n",
       " \"The table compares the performance of the RAG model against Thorne and Vlachos' model in a 2-way classification task, where RoBERTa is trained to classify a claim as true or false. Despite only being supplied with the claim and retrieving its own evidence, RAG's accuracy is within 2.7% of Thorne and Vlachos' model. The table also analyzes the overlap between documents retrieved by RAG and those annotated as gold evidence in FEVER. It shows that in 71% of cases, the top retrieved document is from a gold article, and in 90% of cases, a gold article is present in the top 10 retrieved articles.\",\n",
       " \"The table presents a comparison of generation diversity among RAG models and BART in Jeopardy question generation. It highlights that RAG-Sequence's generations are more diverse than RAG-Token's, and both surpass BART in diversity without the need for diversity-promoting decoding. The diversity is measured by the ratio of distinct ngrams to total ngrams generated by each model.\",\n",
       " 'The table, \"Retrieval Ablations\", evaluates the effectiveness of the retrieval mechanism in RAG by running ablations with a frozen retriever during training. The results, as depicted in Table 6, indicate that learned retrieval enhances outcomes for all tasks.',\n",
       " \"The table compares the performance of RAG's dense retriever and a word overlap-based BM25 retriever. It shows that the BM25 system performs best for FEVER, likely due to its entity-centric claims. However, differentiable retrieval improves results on all other tasks, particularly in Open-Domain QA.\",\n",
       " 'The table compares the performance of non-parametric memory models like RAG and parametric-only models like T5 or BART in terms of their adaptability to updated information. It uses an example of an index built using the DrQA Wikipedia dump from December 2016 and compares the outputs from RAG using this index to a newer index from December 2018. The comparison is based on a list of 82 world leaders who had changed.',\n",
       " 'The table, \"Table 4: Human assessments for the Jeopardy Question Generation Task,\" provides detailed information on the evaluations made by humans on the task of generating questions for the game show Jeopardy. It likely includes metrics such as accuracy, relevance, and difficulty of the generated questions.',\n",
       " \"The table presents the accuracy of the NQ RAG model in identifying world leaders based on the year of the index used. The model performs best when the index year matches the year of the world leaders, with 70% accuracy for 2016 and 68% for 2018. However, the accuracy drops significantly when the index year and the year of the leaders do not match, with only 12% accuracy for 2018 index with 2016 leaders and 4% for 2016 index with 2018 leaders. The data suggests that updating RAG's non-parametric memory can improve its world knowledge.\",\n",
       " 'The table presents the impact of retrieving more documents on the performance of models trained with either 5 or 10 latent documents. It shows that increasing the number of retrieved documents at test time improves Open-domain QA results for RAG-Sequence, with performance peaking for RAG-Token at 10 documents. However, retrieving more documents increases Rouge-L for RAG-Token at the cost of Bleu-1, with a less noticeable effect for RAG-Sequence.',\n",
       " 'The table summarizes the application of retrieval in various Natural Language Processing (NLP) tasks, demonstrating its effectiveness in improving performance. These tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. The table also highlights the unification of these applications into a single retrieval-based architecture, which has shown strong performance across multiple tasks.',\n",
       " 'The table presents a summary of the evolution and advancements in general-purpose architectures for Natural Language Processing (NLP). It highlights the success of single, pre-trained language models in achieving strong performance on various classification tasks, as demonstrated in the GLUE benchmarks. The table further discusses the development of GPT-2, which showed that a single, left-to-right, pre-trained language model could perform well across both discriminative and generative tasks. The table also mentions the introduction of BART and T5, which propose a single, pre-trained encoder-decoder model that uses bi-directional attention for improved performance. The table concludes with the aim of the current work, which is to expand the range of possible tasks with a unified architecture by learning a retrieval module to augment pre-trained, generative language models.',\n",
       " 'The table discusses the significant work done on learning to retrieve documents in information retrieval, particularly with the use of pre-trained, neural language models. It highlights different optimization techniques used to enhance the retrieval module for specific tasks like question answering, using search, reinforcement learning, or a latent variable approach. The table emphasizes the success of these techniques in improving performance on single tasks, while also noting that a single retrieval-based architecture can be fine-tuned for strong performance across various tasks.',\n",
       " \"The table discusses memory-based architectures in the context of neural networks. It highlights the use of a document index as a large external memory for these networks. The table also mentions concurrent work that focuses on retrieving trained embeddings for each entity in the input, as opposed to retrieving raw text. It further discusses improvements in dialog models' ability to generate factual text by attending over fact embeddings. A key feature of the memory, as per the table, is its composition of raw text rather than distributed representations, making it human-readable and writable. The table also mentions the use of this approach in knowledge-intensive dialog, where generators are conditioned on retrieved text.\",\n",
       " 'The table discusses the similarities and differences between the Retrieve-and-Edit approach and the method in question. It highlights that both methods involve retrieving a similar training input-output pair for a given input. However, the method in question focuses less on lightly editing a retrieved item and more on aggregating content from multiple retrieved pieces. It also emphasizes learning latent retrieval and retrieving evidence documents instead of related training pairs. The table suggests that RAG techniques could be effective in these settings and could be a promising area for future research. The Retrieve-and-Edit approach has been successful in domains like Machine Translation and Semantic Parsing.',\n",
       " 'The table presents a study on hybrid generation models with both parametric and non-parametric memory. The RAG models showcased in the study achieved top results in open-domain QA, with users finding them more factual and specific than the purely parametric BART. The study also validated the effectiveness of the learned retrieval component and demonstrated the ability to update the model without retraining. The research suggests potential for joint pre-training of the two components and opens up new research directions on the interaction and effective combination of parametric and non-parametric memories in various NLP tasks.',\n",
       " 'The table discusses the societal benefits of a work that is grounded in factual knowledge, specifically Wikipedia. It highlights the reduction in \"hallucination\" and increased control and interpretability. The table also suggests potential applications of the work, such as using a medical index for open-domain questions or improving job efficiency.',\n",
       " 'The table discusses potential downsides of using Wikipedia or any external knowledge source, highlighting the risk of misinformation and bias. It also mentions the potential misuse of RAG as a language model, similar to GPT-2, in generating abusive, fake, or misleading content, impersonation, and spam/phishing content automation. The table further suggests that advanced language models could automate various jobs, posing employment risks. However, it proposes the use of AI systems to combat misleading content and automated spam/phishing as a mitigation strategy.',\n",
       " 'The table acknowledges the contributions of reviewers, HuggingFace, Kyunghyun Cho, and Sewon Min towards the paper and the open-sourcing of RAG models. It also mentions the support received by EP from the NSF Graduate Research Fellowship and PL from the FAIR PhD program.',\n",
       " 'The table contains information about a publication in the field of Computational Linguistics. The details include the page numbers (6086-6096), the location of the conference (Florence, Italy), the date (July 2019), the organizing body (Association for Computational Linguistics), the DOI reference (10.18653/v1/P19-1612), and the URL for the publication (https://www.aclweb.org/anthology/P19-1612).',\n",
       " 'The table pertains to the proceedings of the 2016 co-located approaches at the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016) held in Barcelona, Spain on December 9, 2016. The information is documented in volume 1773 of CEUR Workshop Proceedings and is available online at CEUR-WS.org.',\n",
       " 'The table presents test results for Open-domain QA, Open-MSMarco, and Jeopardy question generation using RAG-Token and RAG-Sequence models. For Open-domain QA, 15 documents were retrieved for RAG-Token models and 50 for RAG-Sequence models, using Thorough Decoding and greedy decoding. For Open-MSMarco and Jeopardy, ten documents were retrieved for both models, with a BART-large model used as a baseline. A beam size of four was used, and Fast Decoding was applied for RAG-Sequence models.',\n",
       " 'The table, Figure 4, presents the annotation interface used for human evaluation of factuality. It includes a feature that provides detailed instructions and a worked example when the \"view tool guide\" option is clicked.',\n",
       " 'The table in Figure 4 presents the user interface for human evaluation, detailing the process of random selection for sentence A and B to prevent screen position bias. It mentions the encouragement for annotators to conduct online research and the provision of detailed instructions and examples. The table also discusses the inclusion of gold sentences to assess annotator accuracy, and the removal of annotations from two underperforming annotators.',\n",
       " \"The table details the training process of RAG models and BART baselines using Fairseq, with mixed precision floating point arithmetic and distribution across 8, 32GB NVIDIA V100 GPUs. It mentions the use of Maximum Inner Product Search with FAISS for document index vectors storage on CPU, requiring around 100 GB of CPU memory for all of Wikipedia. The table also notes the porting of the code to HuggingFace Transformers, which is open-sourced and offers equivalent performance with a cleaner implementation. The document index's CPU memory requirement is reduced to 36GB using FAISS's compression tools. Links for running experiments with RAG and an interactive RAG model demo are provided.\",\n",
       " \"The table contains links to two GitHub repositories: PyTorch's 'fairseq', a general-purpose sequence-to-sequence library, and Hugging Face's 'transformers', a state-of-the-art natural language processing library.\",\n",
       " 'The table discusses the use of multiple answer annotations in open-domain QA for training extractive models. It mentions the use of multiple annotation examples for Natural Questions and WebQuestions in training the RAG model, which leads to a slight accuracy increase. The table also talks about TriviaQA, where many valid answers exist for a question, but some are not suitable for training, like emojis or spelling variants. For TriviaQA, answer candidates not appearing in the top 1000 documents for the query are filtered out.',\n",
       " 'The table discusses the preprocessing method for CuratedTrec, which involves retrieving the top 1000 documents for each query and using the most frequently matching regex pattern as the supervision target. If no matches are found, a heuristic is used to generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace. This process is used to make CuratedTrec suitable for answer-generation models.',\n",
       " 'The table presents an evaluation of TriviaQA setups, comparing the use of different test datasets. The common practice in the open-domain QA community is to use public development datasets as test datasets, with the TriviaQA Web Development split being the standard. However, some researchers, like Roberts et al., use the official TriviaQA Wikipedia test set. The table reports results from both test sets for fair comparison. It is found that performance is higher when using the official Wiki test set, attributed to its questions being simpler to answer from Wikipedia.',\n",
       " 'The table discusses the process of FEVER classification, which involves regenerating a claim and classifying it using the final hidden state representation. The classification results in probabilities for three classes: \"Supported\", \"Refuted\", or \"Not Enough Info\". The table also mentions a sub-task of extracting supporting evidence from Wikipedia, which presents a challenge due to the use of a different Wikipedia dump. Future work is planned to address this issue.',\n",
       " 'The table presents an experiment on adding a \"Null document\" mechanism to RAG, similar to REALM. The mechanism involves retrieving an empty document when no useful information can be retrieved for a given input. Three methods were explored to model this null document logit: a document embedding, a static learnt bias term, or a neural network. However, none of these methods improved performance. In the Open MS-MARCO context, the model tends to retrieve a specific set of documents for less beneficial questions, suggesting that null document mechanisms may not be necessary for RAG.',\n",
       " 'The table presents information about the RAG models, detailing the trainable parameters for the BERT-base query and document encoder of DPR, each containing 110M parameters. It also mentions that the document encoder is not trained by the creators. Additionally, it includes data about BART-large, which has 406M trainable parameters. The total number of trainable parameters across all models is 626M.',\n",
       " 'The table provides a comparison of different models for open-domain QA performance. The best performing \"closed-book\" model is T5-11B with 11 billion trainable parameters. T5-large, with 770M parameters, scores 28.9 EM on Natural Questions, significantly lower than the 44.5 achieved by RAG-Sequence, suggesting that hybrid models require fewer parameters for strong performance. The non-parametric memory index, while not having trainable parameters, consists of 21M 728-dimensional vectors, or 15.3B values, which can be stored at 8-bit floating point precision for efficient memory and disk usage.',\n",
       " 'The table discusses the issues observed in preliminary experiments with the retrieval component in tasks like story generation. It notes that the retrieval component would often \"collapse\" and retrieve the same documents regardless of input. This led to the generator ignoring the documents and the RAG model performing similarly to BART. The collapse could be due to a less explicit need for factual knowledge in some tasks or longer target sequences resulting in less informative gradients for the retriever. The table also references Perez et al.\\'s findings of spurious retrieval results when optimizing a retrieval component for improved performance on downstream tasks.',\n",
       " 'The table provides a detailed breakdown of the number of training, development, and test datapoints in various datasets as presented in Table 7.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath=\"extracted_data2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quiz interface image asking which sentence is more factually true about Hemingway. Sentence A states \"The Sun Also Rises\" is by the author of \"A Farewell to Arms.\" Sentence B claims the author was born in Havana, Cuba, the son of Spanish immigrants. Options to select: Sentence A is more true, Sentence B is more true, Both sentences are true, Both sentences are completely untrue. Instructions note the importance of accuracy in control questions.',\n",
       " 'Flowchart diagram of a machine learning model for end-to-end retrieval and generation. It features a query encoder, retriever, document index, and generator. The process includes question answering, fact verification, and Jeopardy question generation. Arrows indicate data flow through components labeled as query encoder, retriever (non-parametric), and generator (parametric). Example queries include a fact about Barack Obama and \"The Divine Comedy.\"',\n",
       " 'Heatmap visualization of document relevance, comparing phrases from \"The Sun Also Rises\" and \"A Farewell to Arms\" with word frequency across five documents. Darker cells indicate higher frequency of matching terms, aligned with specific words on the horizontal axis.',\n",
       " 'Line graphs comparing RAG-Tok and RAG-Seq performance metrics (NQ Exact Match, NQ Answer Recall, Bleu/RougeL score) across varying K Retrieved Docs. Includes additional lines for Fixed DPR and BM25.',\n",
       " 'Table displaying dataset sizes for various tasks: Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy Question Generation, MS-MARCO, FEVER-3-way, and FEVER-2-way. Columns include Train, Development, and Test sets, with numbers listed for each task.',\n",
       " 'A table comparing performance of various models on different datasets and tasks. Models are divided into Closed Book and Open Book categories, including T5-11B, REALM, DPR, RAG-Token, and RAG-Seq. Performance metrics are provided for datasets like NQ, TQA, WQ, CT, Jeopardy, and MS MARCO with metrics like B-1, R-L, and accuracy scores indicated. Notable scores are bolded or underlined.',\n",
       " 'Table comparing different models (BART, RAG-T, RAG-S) for generating text based on specific tasks from MS-MARCO and Jeopardy questions. Includes tasks like defining \"middle ear,\" currency needed in Scotland, identifying the state of Washington, and describing \"The Divine Comedy\" by Dante.',\n",
       " 'Comparison tables of BART and RAG models in terms of factuality and specificity, and performance on MSMARCO and Jeopardy QGen datasets.',\n",
       " 'Table comparing performance metrics of different RAG models. Models listed are RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, and RAG-Sequence. Metrics include NQ, TQA Exact Match, WQ, CT, Jeopardy-QGen (B-1, QB-1), MSMarco (R-L, B-1), and FVR-3 Label Accuracy. RAG-Sequence shows the highest performance in several categories like NQ and MSMarco R-L. RAG-Token achieves the highest in TQA Exact Match. Key highlighted values include FVR-3 Label Accuracy for RAG-Token-BM25 at 75.1 and RAG-Sequence at 57.2 MSMarco R-L.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a MultiVector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_27732\\1848821793.py:44: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "\n",
    "      doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "\n",
    "      summary_docs = [\n",
    "              Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "              for i, s in enumerate(doc_summaries)\n",
    "          ]\n",
    "\n",
    "      retriever.vectorstore.add_documents(summary_docs)\n",
    "      retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "      # Add texts, tables, and images\n",
    "      # Check that text_summaries is not empty before adding\n",
    "      if text_summaries:\n",
    "          add_documents(retriever, text_summaries, texts)\n",
    "      # Check that table_summaries is not empty before adding\n",
    "      if table_summaries:\n",
    "          add_documents(retriever, table_summaries, tab)\n",
    "      # Check that image_summaries is not empty before adding\n",
    "      if image_summaries:\n",
    "          add_documents(retriever, image_summaries, img)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    Text,\n",
    "    table_summaries,\n",
    "    Table,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001D6AD5375E0>, docstore=<langchain_core.stores.InMemoryStore object at 0x000001D6BB95D990>, search_kwargs={})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_multi_vector_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f''\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flowchart diagram of a machine learning model for end-to-end retrieval and generation. It features a query encoder, retriever, document index, and generator. The process includes question answering, fact verification, and Jeopardy question generation. Arrows indicate data flow through components labeled as query encoder, retriever (non-parametric), and generator (parametric). Example queries include a fact about Barack Obama and \"The Divine Comedy.\"'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    #print(data_dict)\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"You will be given a mixed info(s) .\\n\"\n",
    "            \"Use this information to provide relevant information to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001D6AD5375E0>, docstore=<langchain_core.stores.InMemoryStore object at 0x000001D6BB95D990>, search_kwargs={})\n",
       "           | RunnableLambda(split_image_text_types),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| RunnableLambda(img_prompt_func)\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D6C18FD930>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D6C18FF4F0>, root_client=<openai.OpenAI object at 0x000001D6BF757AC0>, root_async_client=<openai.AsyncOpenAI object at 0x000001D6C18FDBA0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=1024)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_multimodal_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Why We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end?\"\n",
    "docs = retriever_multi_vector_img.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Open-Domain QA Test Scores. For TQA,\\\n",
    "left column uses the standard test set for Open-\\\n",
    "Domain QA, right column uses the TQA-Wiki\\\n",
    "test set. See Appendix D for further details.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever_multi_vector_img.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Models are trained with either 5 or 10 retrieved latent\\\n",
    "documents, and we do not observe significant differences in performance between them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_multi_vector_img.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We get back relevant images\n",
    "plt_img_base64(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"can you explain me this Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
    "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1=\"Explain any images / figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
    "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems like the paper you are referring to includes figures that illustrate the performance of certain models or systems on specific tasks as more documents are retrieved. Here's a general explanation of what each part of the figure might represent based on the descriptions you provided:\\n\\n1. **Left: NQ performance as more documents are retrieved**:\\n   - This part of the figure likely shows how the performance of a model on the Natural Questions (NQ) dataset changes as the number of documents retrieved increases. The NQ dataset is commonly used for training and evaluating question-answering systems.\\n   - The x-axis might represent the number of documents retrieved, while the y-axis could represent a performance metric such as accuracy, F1 score, or another relevant measure.\\n   - The curve or line in the graph would show whether performance improves, plateaus, or declines as more documents are considered.\\n\\n2. **Center: Retrieval recall performance in NQ**:\\n   - This figure likely focuses on the recall performance of the retrieval system when applied to the NQ dataset.\\n   - Recall is a metric that measures the ability of the system to retrieve all relevant documents. A higher recall indicates that the system is successfully retrieving a larger proportion of the relevant documents.\\n   - The x-axis might again represent the number of documents retrieved, while the y-axis would represent the recall score.\\n   - The graph would show how recall changes as more documents are retrieved, which can help in understanding the trade-off between retrieving more documents and maintaining high recall.\\n\\n3. **Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved**:\\n   - This part of the figure likely illustrates the performance of a model on the MS-MARCO dataset, which is another benchmark for evaluating information retrieval and question-answering systems.\\n   - BLEU-1 and ROUGE-L are metrics used to evaluate the quality of text generation or summarization. BLEU-1 measures the precision of unigrams, while ROUGE-L measures the longest common subsequence between the generated text and reference text.\\n   - The x-axis might represent the number of documents retrieved, while the y-axis could show the scores for BLEU-1 and ROUGE-L.\\n   - The graph would indicate how these scores change as more documents are retrieved, providing insights into the quality of the generated answers or summaries as the retrieval set grows.\\n\\nThese explanations are based on typical interpretations of such figures in academic papers. For precise details, it would be necessary to refer to the specific paper and its context.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "chain_multimodal_rag.invoke(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
